{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing distances for graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from typing import Tuple\n",
    "from geopy.distance import geodesic\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import normalize\n",
    "from enum import Enum\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch_geometric_temporal.nn.attention.stgcn import STConv\n",
    "from torch_geometric_temporal.nn.recurrent import GCLSTM, DCRNN\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import ReLU, Linear, Module, BatchNorm1d, Dropout\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants needed for data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data\"\n",
    "GRAPH_INFO_TXT = \"d07_text_meta_2021_03_27.txt\"\n",
    "current_directory = os.getcwd()\n",
    "path_raw_data = os.path.join(current_directory, DATA_FOLDER)\n",
    "path_raw_data_graph_info_txt = os.path.join(path_raw_data, GRAPH_INFO_TXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set total number of nodes (nb_days) from Metadata (may contain empty nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_get_number_of_nodes(path_raw_data_graph_info_txt):\n",
    "    nodes_location = []\n",
    "    skip = True\n",
    "    with open(path_raw_data_graph_info_txt) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            if skip:\n",
    "                skip = False\n",
    "            else:\n",
    "                line = line.split('\\t')\n",
    "                line = line[:-1]         # ID     #LAT     #LONG\n",
    "                nodes_location.append([line[0], line[8], line[9]])\n",
    "\n",
    "    return len(nodes_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of nodes are : 4904\n"
     ]
    }
   ],
   "source": [
    "total_num_nodes = DataReader_get_number_of_nodes(path_raw_data_graph_info_txt)\n",
    "print(f'The total number of nodes are : {total_num_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nodes that have data as good_nodes and nodes that have empty data as empty_nodes from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_get_good_empty_nodes(path_raw_data,total_num_nodes):\n",
    "    index = total_num_nodes\n",
    "    empty_nodes = []\n",
    "    good_nodes = []\n",
    "    txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "    for file in glob(txtFiles):\n",
    "        with open(file) as f:\n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                line = line.split(',')\n",
    "                line = [line1.replace(\"\\n\", \"\") for line1 in line]\n",
    "                if not (line[9] == '' or line[10] == '' or line[11] == ''):\n",
    "                    good_nodes.append((int)(line[1]))\n",
    "                else:\n",
    "                    empty_nodes.append((int)(line[1]))\n",
    "                index -= 1\n",
    "                if index == 0:\n",
    "                    return (good_nodes,empty_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five examples of nodes that contain data: [715898, 715918, 715920, 715929, 715930]\n",
      "Five examples of nodes that do not have data: [715900, 715901, 715903, 715904, 715905]\n"
     ]
    }
   ],
   "source": [
    "(good_nodes, empty_nodes) = DataReader_get_good_empty_nodes(\n",
    "    path_raw_data, total_num_nodes)\n",
    "print(f'Five examples of nodes that contain data: {good_nodes[:5]}')\n",
    "print(f'Five examples of nodes that do not have data: {empty_nodes[:5]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data and labels as X and Y variables from Data. They will contain good nodes and empty nodes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_read_data(path_raw_data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "    nb_days = 0\n",
    "    for file in glob(txtFiles):\n",
    "        with open(file) as f:\n",
    "            print(f'Reading day {nb_days + 1}')\n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                line = line.split(',')\n",
    "                line = [line1.replace(\"\\n\", \"\") for line1 in line]\n",
    "                if not (line[9] == '' or line[10] == '' or line[11] == ''):\n",
    "                    Y.append((float)(line[11]))\n",
    "                    X.append([(float)(line[9]), (float)(line[10])])\n",
    "        nb_days += 1\n",
    "        # TODO : code for debugging, delete when finished\n",
    "        # ------------------------------------------------\n",
    "        # if nb_days == 2:\n",
    "        #     break\n",
    "        # ------------------------------------------------\n",
    "    X = normalize(np.array(X))\n",
    "    Y = Y\n",
    "    return X,Y,nb_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading day 1\n",
      "Reading day 2\n",
      "Reading day 3\n",
      "Reading day 4\n",
      "Reading day 5\n",
      "Reading day 6\n",
      "Reading day 7\n",
      "Reading day 8\n",
      "Reading day 9\n",
      "Reading day 10\n",
      "Reading day 11\n",
      "Reading day 12\n",
      "Reading day 13\n",
      "Reading day 14\n",
      "Reading day 15\n",
      "Reading day 16\n",
      "Reading day 17\n",
      "Reading day 18\n",
      "Reading day 19\n",
      "Reading day 20\n",
      "Reading day 21\n",
      "Reading day 22\n"
     ]
    }
   ],
   "source": [
    "X_data,Y_data,nb_days = DataReader_read_data(path_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days read: 22\n",
      "Five examples of normalized input data: [[9.99999984e-01 1.76732671e-04]\n",
      " [9.99999978e-01 2.08088231e-04]\n",
      " [9.99999974e-01 2.29906536e-04]\n",
      " [9.99999979e-01 2.03723400e-04]\n",
      " [9.99999976e-01 2.17045449e-04]]\n",
      "Five examples of input data label: [70.4, 69.2, 66.0, 72.5, 69.8]\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of days read: {nb_days}')\n",
    "print(f'Five examples of normalized input data: {X_data[:5]}')\n",
    "print(f'Five examples of input data label: {Y_data[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nodes geo location from Metadata. They will contain good nodes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_read_nodes_data(path_raw_data_graph_info_txt, good_nodes):\n",
    "\n",
    "    nodes_location = []\n",
    "    skip = True\n",
    "    with open(path_raw_data_graph_info_txt) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            if skip:\n",
    "                skip = False\n",
    "            else:\n",
    "                line = line.split('\\t')\n",
    "                line = line[:-1]\n",
    "                if (int)(line[0]) in good_nodes:  # ID  #LAT    #LONG\n",
    "                    nodes_location.append([line[0], line[8], line[9]])\n",
    "    return nodes_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five examples of nodes geolocation as well as their ID's [['715898', '33.880183', '-118.021787'], ['715918', '33.93311', '-118.091005'], ['715920', '33.938544', '-118.094941'], ['715929', '33.971707', '-118.123095'], ['715930', '33.971763', '-118.122905']]\n"
     ]
    }
   ],
   "source": [
    "nodes_location = DataReader_read_nodes_data(\n",
    "    path_raw_data_graph_info_txt, good_nodes)\n",
    "print(\n",
    "    f'Five examples of nodes geolocation as well as their ID\\'s {nodes_location[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(path_raw_data,graph_info_txt):\n",
    "        columnsInfo = [\n",
    "            'Timestamp', 'Station', 'District', 'Freeway', 'DirOfTravel',\n",
    "            'LaneType', 'Length', 'Samples', 'Observed', 'Flow', 'Occupancy',\n",
    "            'Speed'\n",
    "        ]\n",
    "        for i in range(1, 9):\n",
    "            columnsInfo.extend([\n",
    "                str(i) + '_Samples',\n",
    "                str(i) + '_Flow',\n",
    "                str(i) + '_Occupancy',\n",
    "                str(i) + '_Speed',\n",
    "                str(i) + '_Observed'\n",
    "            ])\n",
    "        columnsMetadata = [\n",
    "            'ID', 'Fwy', 'Dir', 'District', 'County', 'City', 'State_PM',\n",
    "            'Abs_PM', 'Latitude', 'Longitude', 'Length', 'Type', 'Lanes',\n",
    "            'Name', 'User_ID_1', 'User_ID_2', 'User_ID_3', 'User_ID_4'\n",
    "        ]\n",
    "        txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "        print(\"Reading Metadata\")\n",
    "        dataframeMetadata = pd.read_csv(path_raw_data_graph_info_txt,\n",
    "                                        sep='\\t',\n",
    "                                        skiprows=1,\n",
    "                                        header=None,\n",
    "                                        names=columnsMetadata)\n",
    "        print(\"Finished Reading Metadata\")\n",
    "        print(\"Reading Information\")\n",
    "        nb_days = 0\n",
    "        dataframeInfo = pd.DataFrame(columns=columnsInfo)\n",
    "\n",
    "        for file in glob(txtFiles):\n",
    "            print(\"Reading day {0}\".format(nb_days + 1))\n",
    "            with open(file) as f:\n",
    "                dataframeInfo = dataframeInfo.append(pd.read_csv(\n",
    "                    file, sep=',', header=None, names=columnsInfo),\n",
    "                                                     ignore_index=True)\n",
    "                day = None\n",
    "                nb_days += 1\n",
    "\n",
    "            if nb_days == 5:\n",
    "                break\n",
    "\n",
    "        print(\"Finished Reading Information\")\n",
    "        return dataframeInfo, dataframeMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Metadata\n",
      "Finished Reading Metadata\n",
      "Reading Information\n",
      "Reading day 1\n",
      "Reading day 2\n",
      "Reading day 3\n",
      "Reading day 4\n",
      "Reading day 5\n",
      "Finished Reading Information\n",
      "             Timestamp Station District Freeway DirOfTravel LaneType  Length  \\\n",
      "0  06/01/2021 00:00:00  715898        7       5           S       ML    0.43   \n",
      "1  06/01/2021 00:00:00  715900        7       5           S       OR     NaN   \n",
      "2  06/01/2021 00:00:00  715901        7       5           N       OR     NaN   \n",
      "3  06/01/2021 00:00:00  715903        7       5           N       OR     NaN   \n",
      "4  06/01/2021 00:00:00  715904        7       5           S       OR     NaN   \n",
      "\n",
      "  Samples Observed   Flow  ...  7_Samples  7_Flow  7_Occupancy  7_Speed  \\\n",
      "0       0        0  202.0  ...        NaN     NaN          NaN      NaN   \n",
      "1       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "2       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "3       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "4       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "\n",
      "   7_Observed  8_Samples 8_Flow  8_Occupancy  8_Speed  8_Observed  \n",
      "0           0        NaN    NaN          NaN      NaN           0  \n",
      "1           0        NaN    NaN          NaN      NaN           0  \n",
      "2           0        NaN    NaN          NaN      NaN           0  \n",
      "3           0        NaN    NaN          NaN      NaN           0  \n",
      "4           0        NaN    NaN          NaN      NaN           0  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "       ID  Fwy Dir  District  County     City State_PM   Abs_PM   Latitude  \\\n",
      "0  715898    5   S         7      37  40032.0      .71  117.280  33.880183   \n",
      "1  715900    5   S         7      37  40032.0     1.06  117.630  33.882892   \n",
      "2  715901    5   N         7      37  40032.0     1.11  117.743  33.883400   \n",
      "3  715903    5   N         7      37  69154.0     1.56  118.193  33.886992   \n",
      "4  715904    5   S         7      37  69154.0     2.27  118.840  33.892489   \n",
      "\n",
      "    Longitude  Length Type  Lanes         Name  User_ID_1  User_ID_2  \\\n",
      "0 -118.021787    0.43   ML      3       PHOEBE       2029        NaN   \n",
      "1 -118.026822     NaN   OR      1  VALLEY VIEW       3255        NaN   \n",
      "2 -118.027451     NaN   OR      1  VALLEY VIEW       3268        NaN   \n",
      "3 -118.034125     NaN   OR      1      ALONDRA       3269        NaN   \n",
      "4 -118.044573     NaN   OR      1    CARMENITA       3253        NaN   \n",
      "\n",
      "   User_ID_3  User_ID_4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "path_plots = os.path.join(current_directory,\"Plots\")\n",
    "path_plots_general = os.path.join(path_plots,\"General\")\n",
    "dfInfo,dfMetaData = visualization(path_raw_data,path_raw_data_graph_info_txt)\n",
    "print(dfInfo[:5])\n",
    "print(dfMetaData[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PieChartRoadwayType(dfMeta):\n",
    "\n",
    "    df = dfMeta\n",
    "    series = df['Type'].value_counts(ascending=False, dropna=True)\n",
    "    dataframe = pd.DataFrame({\n",
    "        'Type': series.index,\n",
    "        'count': series.values\n",
    "    })\n",
    "    dataframe = dataframe.replace({\n",
    "        'CD': 'Coll/Dist',\n",
    "        'CH': 'Conventional Highway',\n",
    "        'FF': 'Freeway-Freeway connector',\n",
    "        'FR': 'Off Ramp',\n",
    "        'HV': 'HOV',\n",
    "        'ML': 'Mainline',\n",
    "        'OR': 'On Ramp'\n",
    "    })\n",
    "    fig = px.pie(dataframe,\n",
    "                    values='count',\n",
    "                    names='Type',\n",
    "                    title='Types of roads')\n",
    "                    \n",
    "    iplot(fig, filename=f'PieChartRoadwayType')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hovertemplate": "Type=%{label}<br>count=%{value}<extra></extra>",
         "labels": [
          "Mainline",
          "On Ramp",
          "HOV",
          "Off Ramp",
          "Freeway-Freeway connector",
          "Coll/Dist"
         ],
         "legendgroup": "",
         "name": "",
         "showlegend": true,
         "type": "pie",
         "values": [
          1923,
          1090,
          866,
          783,
          210,
          32
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Types of roads"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"6cc9bb88-8a8a-42e2-ac11-3f0250a7130b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6cc9bb88-8a8a-42e2-ac11-3f0250a7130b\")) {                    Plotly.newPlot(                        \"6cc9bb88-8a8a-42e2-ac11-3f0250a7130b\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"Type=%{label}<br>count=%{value}<extra></extra>\",\"labels\":[\"Mainline\",\"On Ramp\",\"HOV\",\"Off Ramp\",\"Freeway-Freeway connector\",\"Coll/Dist\"],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"type\":\"pie\",\"values\":[1923,1090,866,783,210,32]}],                        {\"legend\":{\"tracegroupgap\":0},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Types of roads\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6cc9bb88-8a8a-42e2-ac11-3f0250a7130b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PieChartRoadwayType(dfMetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlotSpeed(dfInfo) -> None:\n",
    "\n",
    "    df = dfInfo\n",
    "    df = df[df['Speed'].notna()]\n",
    "    df['Day'] = df['Timestamp'].apply(lambda x: datetime.strptime(\n",
    "        x, '%m/%d/%Y %H:%M:%S').weekday())\n",
    "    fig = px.box(df, x='Day', y='Speed')\n",
    "    iplot(fig, filename='BoxPlotSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoxPlotSpeed(dfInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapHeatmapSpeed(dfInfo,dfMeta,hour) -> None:\n",
    "\n",
    "    dfInfoFunc = dfInfo\n",
    "    dfMetaFunc = dfMeta\n",
    "\n",
    "    dfInfo2 = dfInfoFunc[dfInfoFunc['Speed'].notna()]\n",
    "    dfInfo2['Hour'] = dfInfo2['Timestamp'].apply(\n",
    "        lambda x: datetime.datetime.strptime(x, '%m/%d/%Y %H:%M:%S').hour)\n",
    "    dfInfo2 = dfInfo2.loc[dfInfo2['Hour'] == hour]\n",
    "    dfInfo2 = dfInfo2[['Hour', 'Station', 'Speed']]\n",
    "    dfInfo2 = dfInfo2.groupby(['Hour', 'Station']).mean()\n",
    "    df = pd.merge(dfMetaFunc, dfInfo2, left_on='ID', right_on='Station')\n",
    "    fig = px.scatter_mapbox(\n",
    "        df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=\"Speed\",\n",
    "        color_continuous_scale=px.colors.sequential.Bluered,\n",
    "        zoom=8,\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        title='Traffic speed at {0}:00'.format(str(hour)))\n",
    "    iplot(fig, filename=f'MapHeatmapSpeed_{hour}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMapHeatmapSpeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdfMetaData\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [18], line 7\u001b[0m, in \u001b[0;36mMapHeatmapSpeed\u001b[1;34m(dfInfo, dfMeta, hour)\u001b[0m\n\u001b[0;32m      4\u001b[0m dfMetaFunc \u001b[38;5;241m=\u001b[39m dfMeta\n\u001b[0;32m      6\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfoFunc[dfInfoFunc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\n\u001b[1;32m----> 7\u001b[0m dfInfo2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdfInfo2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm/\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfo2\u001b[38;5;241m.\u001b[39mloc[dfInfo2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m hour]\n\u001b[0;32m     10\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfo2[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Disertatie\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Disertatie\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Disertatie\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1099\u001b[0m             values,\n\u001b[0;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1102\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Disertatie\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn [18], line 8\u001b[0m, in \u001b[0;36mMapHeatmapSpeed.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m dfMetaFunc \u001b[38;5;241m=\u001b[39m dfMeta\n\u001b[0;32m      6\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfoFunc[dfInfoFunc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\n\u001b[0;32m      7\u001b[0m dfInfo2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dfInfo2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mstrptime(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhour)\n\u001b[0;32m      9\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfo2\u001b[38;5;241m.\u001b[39mloc[dfInfo2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m hour]\n\u001b[0;32m     10\u001b[0m dfInfo2 \u001b[38;5;241m=\u001b[39m dfInfo2[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "MapHeatmapSpeed(dfInfo,dfMetaData,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MapHeatmapSpeed(dfInfo,dfMetaData,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MapHeatmapSpeed(dfInfo,dfMetaData,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MapHeatmapSpeed(dfInfo,dfMetaData,22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumartion used for Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Enum):\n",
    "    Experimental = 0\n",
    "    ExperimentalManual = 1\n",
    "    ExperimentalLR = 2\n",
    "    Tiny = 3\n",
    "    TinyManual = 4\n",
    "    TinyLR = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNodes(Enum):\n",
    "    Experimental = [ 718292, 769496, 718291, 718290, 764567, 774279, 774278, 764671 ]\n",
    "    Tiny = [\n",
    "        775637, 718165, 776986, 759289, 774672, 760643, 774671, 717046, 718419,\n",
    "        769105, 764026, 759280, 775636, 759385, 760635, 718166, 774685, 774658,\n",
    "        716938, 776177, 763453, 718421, 717045, 768598, 717043, 716063, 717041,\n",
    "        717040, 717039, 737184, 717042, 718335, 763458, 776981, 737158, 737313,\n",
    "        769118, 772501, 718173, 764037, 763447, 763246, 718041, 763251, 763424,\n",
    "        763429, 763434, 763439, 764032, 764418\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSize(Enum):\n",
    "    Experimental = len(DatasetNodes.Experimental.value)\n",
    "    Tiny = len(DatasetNodes.Tiny.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to set the nodes for the datasets such that they are the same throughout the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More constants defined, as well as the nodes id's for the Experimental and Tiny dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_nodes_for_dataset(dataset):\n",
    "    experimental_all = [Dataset.Experimental,\n",
    "                        Dataset.ExperimentalManual, Dataset.ExperimentalLR]\n",
    "    tiny_all = [Dataset.Tiny, Dataset.TinyManual, Dataset.TinyLR]\n",
    "    if dataset in experimental_all:\n",
    "        return DatasetNodes.Experimental.value\n",
    "    elif dataset in tiny_all:\n",
    "        return DatasetNodes.Tiny.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Graph_get_nodes_for_dataset(Dataset.Experimental))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_number_of_nodes_for_dataset(dataset):\n",
    "    experimental_all = [Dataset.Experimental,\n",
    "                        Dataset.ExperimentalManual, Dataset.ExperimentalLR]\n",
    "    tiny_all = [Dataset.Tiny, Dataset.TinyManual, Dataset.TinyLR]\n",
    "    if dataset in experimental_all:\n",
    "        return DatasetSize.Experimental.value\n",
    "    elif dataset in tiny_all:\n",
    "        return DatasetSize.Tiny.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapPlotSensors(dataset,dfMeta):\n",
    "\n",
    "    df = dfMeta\n",
    "    zoom = 8\n",
    "    datanodes = Graph_get_number_of_nodes_for_dataset(dataset)\n",
    "    df = df[df['ID'].isin(datanodes)]\n",
    "    if dataset == DatasetSize.Experimental:\n",
    "        zoom = 14\n",
    "    if dataset == DatasetSize.Tiny:\n",
    "        zoom = 12\n",
    "    fig = px.scatter_mapbox(df,\n",
    "                            lat=\"Latitude\",\n",
    "                            lon=\"Longitude\",\n",
    "                            hover_name=\"ID\",\n",
    "                            hover_data=[\"Type\", \"Lanes\"],\n",
    "                            color_discrete_sequence=[\"black\"],\n",
    "                            zoom=zoom,\n",
    "                            size_max=15,\n",
    "                            mapbox_style=\"open-street-map\")\n",
    "\n",
    "    iplot(fig, filename=f'BoxPlotSpeed_{dataset.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MapPlotSensors(Dataset.Experimental)\n",
    "MapPlotSensors(Dataset.Tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Graph_get_number_of_nodes_for_dataset(Dataset.Experimental))\n",
    "print(Graph_get_number_of_nodes_for_dataset(Dataset.Tiny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_extract_time(json):\n",
    "    try:\n",
    "        return float(json['routes']['distance'])\n",
    "    except KeyError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_OSRM_loop(p1: tuple, p2: tuple) -> float:\n",
    "    requestUrl = f'http://router.project-osrm.org/route/v1/driving/{p1[1]},{p1[0]};{p2[1]},{p2[0]}'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(requestUrl)\n",
    "    except:\n",
    "        return -1\n",
    "    if (response.status_code != 204\n",
    "            and response.headers[\"content-type\"].strip().startswith(\n",
    "                \"application/json\")):\n",
    "        try:\n",
    "            responseJson = response.json()\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    if responseJson['code'] == \"Ok\":\n",
    "        routes = responseJson['routes']\n",
    "        routes.sort(key=Graph_extract_time, reverse=True)\n",
    "        shortest_distance = float(routes[0]['distance']) * (1 / 1000)\n",
    "        return shortest_distance\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_OSRM(p1: tuple, p2: tuple) -> float:\n",
    "    distance = Graph_OSRM_loop(p1, p2)\n",
    "    while (distance == -1):\n",
    "        distance = Graph_OSRM_loop(p1, p2)\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = ('33.880183', '-118.021787')\n",
    "p2 = ('33.93311', '-118.091005')\n",
    "print(\n",
    "    f'Geodesic distance between node id 715898 and node id 715918 is {geodesic(p1,p2)}')\n",
    "print(\n",
    "    f'Road distance between node id 715898 and node id 715918 is {Graph_OSRM(p1,p2)} km')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_compute_all_OSRM_and_Geodesic(nodes_location, path_distances):\n",
    "    for dataset in [Dataset.Experimental, Dataset.Tiny]:\n",
    "\n",
    "        name_OSRM = os.path.join(\n",
    "            path_distances, f'distances_OSRM_{dataset.name}.npy')\n",
    "        name_geodesic = os.path.join(\n",
    "            path_distances, f'distances_Geodesic_{dataset.name}.npy')\n",
    "\n",
    "        if (os.path.exists(name_OSRM) and os.path.exists(name_geodesic)):\n",
    "            print(f'Distances already computed for {dataset.name}')\n",
    "            continue\n",
    "\n",
    "        nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "        nodes_location_dataset = [node for node in nodes_location if (int)(node[0]) in nodes_ids]\n",
    "\n",
    "        matrix_size = len(nodes_location_dataset)\n",
    "\n",
    "        OSRM_array = np.zeros((matrix_size, matrix_size))\n",
    "        geodesic_array = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        for i in range(matrix_size - 1):\n",
    "            for j in range(i + 1, matrix_size):\n",
    "                p1 = ((float)(nodes_location_dataset[i][1]),\n",
    "                      (float)(nodes_location_dataset[i][2]))\n",
    "                p2 = ((float)(nodes_location_dataset[j][1]),\n",
    "                      (float)(nodes_location_dataset[j][2]))\n",
    "\n",
    "                id_1 = (int)(nodes_location_dataset[i][0])\n",
    "                id_2 = (int)(nodes_location_dataset[j][0])\n",
    "\n",
    "                print(f'Computing distances for Id\\'s {id_1} and {id_2}')\n",
    "\n",
    "                OSRM_array[i][j] = Graph_OSRM(p1, p2)\n",
    "                geodesic_array[i][j] = geodesic(p1, p2).km\n",
    "                OSRM_array[j][i] = Graph_OSRM(p2, p1)\n",
    "                geodesic_array[j][i] = geodesic(p2, p1).km\n",
    "\n",
    "        np.save(name_OSRM, OSRM_array)\n",
    "        np.save(name_geodesic, geodesic_array)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_processed_data = os.path.join(current_directory, \"Processed\")\n",
    "if not os.path.exists(path_processed_data):\n",
    "    os.makedirs(path_processed_data)\n",
    "\n",
    "path_distances = os.path.join(path_processed_data, \"Distances\")\n",
    "if not os.path.exists(path_distances):\n",
    "    os.makedirs(path_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started computing distances...\")\n",
    "Graph_compute_all_OSRM_and_Geodesic(nodes_location, path_distances)\n",
    "print(\"Finished computing distances...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_distances_OSRM_Experimental = np.load(os.path.join(\n",
    "    path_distances, f'distances_OSRM_Experimental.npy'))\n",
    "array_distances_Geodesic_Experimental = np.load(os.path.join(\n",
    "    path_distances, f'distances_Geodesic_Experimental.npy'))\n",
    "print(array_distances_OSRM_Experimental)\n",
    "print(array_distances_Geodesic_Experimental)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceType(Enum):\n",
    "    Geodesic = 0\n",
    "    OSRM = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_adjency_matrix_weight(ID1_index, ID2_index, epsilon, sigma, distanceType,distances_array) -> float:\n",
    "    distance = distances_array[ID1_index][ID2_index]\n",
    "    weight = math.exp(-((distance**2) / (sigma**2)))\n",
    "    if weight >= epsilon:\n",
    "        return weight\n",
    "    else:\n",
    "        return 0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_Experimental_manual = [[0, 1, 7, 4, 7, 5], [1, 2, 4, 3, 6, 4]]\n",
    "edge_index_Tiny_manual = [[\n",
    "    0, 0, 0, 1, 5, 5, 5, 9, 9, 9, 10, 10, 10, 6, 14, 15, 7, 13, 10, 11, 8,\n",
    "    9, 4, 16, 5, 2, 20, 3, 22, 23, 24, 25, 26, 28, 29, 30, 31, 21, 32, 33,\n",
    "    34, 36, 17, 38, 12, 39, 40, 41, 42, 44, 45, 46, 47, 48, 27, 35, 9\n",
    "],\n",
    "    [\n",
    "    1, 2, 3, 4, 6, 7, 2, 4, 3, 12, 12, 11,\n",
    "    4, 14, 15, 12, 13, 10, 11, 8, 2, 3, 16,\n",
    "    17, 6, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "    29, 30, 31, 7, 32, 33, 34, 35, 9, 37,\n",
    "    5, 39, 40, 41, 42, 43, 45, 46, 47, 48,\n",
    "    0, 19, 18, 36\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_Standard(nodes_location, epsilon, sigma, dataset, distanceType,array_distances):\n",
    "    nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    nodes_location_manual = [node for node in nodes_location if (int)(node[0]) in nodes]\n",
    "    num_nodes = len(nodes_location_manual)\n",
    "    for i in range(num_nodes - 1):\n",
    "        for j in range(i, num_nodes - 1):\n",
    "            if i != j:\n",
    "                weight = Graph_get_adjency_matrix_weight(i,j,epsilon, sigma, distanceType,array_distances)\n",
    "                if weight > 0:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(weight)\n",
    "    edge_index = np.transpose(edge_index)\n",
    "    return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_Manual(nodes_location, epsilon, sigma, dataset, distanceType,\n",
    "            edge_index_Experimental_manual,edge_index_Tiny_manual,array_distances):\n",
    "    \n",
    "    edge_weight = []\n",
    "    if dataset == Dataset.ExperimentalManual:\n",
    "        edge_index = edge_index_Experimental_manual\n",
    "    elif dataset == Dataset.TinyManual:\n",
    "        edge_index = edge_index_Tiny_manual\n",
    "    nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "    nodes_location_standard = [node for node in nodes_location if (int)(node[0]) in nodes]\n",
    "    num_nodes = len(nodes_location_standard)\n",
    "    nodes_info = np.zeros((num_nodes, 3))\n",
    "    for i in range(num_nodes):\n",
    "        np.where(nodes == (int)(nodes_location_standard[i][0]))[0]\n",
    "        nodes_info[np.where(nodes == (int)(nodes_location_standard[i][0]))[0]] = nodes_location_standard[i]\n",
    "    nodes_location_standard = np.array(nodes_info)\n",
    "    for i in range(len(edge_index[0])):\n",
    "        weight = Graph_get_adjency_matrix_weight(\n",
    "            edge_index[0][i],edge_index[1][i], epsilon, sigma, distanceType,array_distances\n",
    "        )\n",
    "        edge_weight.append(weight)\n",
    "    return edge_index, edge_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_top_3_nodes_for_node_with_LR(node, dataset, nodes_location, speed_vector):\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    ids_index = 0\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    nodes_used = []\n",
    "    node_ids_order = [(int)(node[0]) for node in nodes_location]\n",
    "    nodes_used_computed = False\n",
    "    X_train_snapshot = []\n",
    "    for speed in speed_vector:\n",
    "\n",
    "        if node != node_ids_order[ids_index] and node_ids_order[ids_index] in nodes_ids:\n",
    "            X_train_snapshot.append(speed)\n",
    "            if not nodes_used_computed:\n",
    "                nodes_used.append(node_ids_order[ids_index])\n",
    "\n",
    "        if node == node_ids_order[ids_index]:\n",
    "            Y_train.append(speed)\n",
    "\n",
    "        ids_index += 1\n",
    "        if ids_index == len(node_ids_order): \n",
    "            ids_index = 0\n",
    "            X_train.append(X_train_snapshot)\n",
    "            X_train_snapshot = []\n",
    "            nodes_used_computed = True\n",
    "\n",
    "    regression = LinearRegression(positive=True).fit(X_train, Y_train)\n",
    "    coeffiecients = regression.coef_.tolist()\n",
    "    results = zip(nodes_used, coeffiecients)\n",
    "    sorted_results = sorted(results, key=lambda tup: tup[1], reverse=True)\n",
    "    best = sorted_results[:3]\n",
    "    return [result[0] for result in best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Graph_get_top_3_nodes_for_node_with_LR(717042, Dataset.TinyLR, nodes_location, Y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_LR(nodes_location, epsilon, sigma, dataset, distanceType,array_distances, Y_data):\n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    for node in nodes_ids:\n",
    "        nodes_relevant = Graph_get_top_3_nodes_for_node_with_LR(node, dataset, nodes_location, Y_data)\n",
    "        for node_relevant in nodes_relevant:\n",
    "            edge_index.append([nodes_ids.index(node_relevant),nodes_ids.index(node)])\n",
    "            edge_weight.append(Graph_get_adjency_matrix_weight(\n",
    "                    nodes_ids.index(node_relevant), nodes_ids.index(node),\n",
    "                    epsilon, sigma, distanceType,array_distances))\n",
    "    edge_index = [list(x) for x in set(tuple(x) for x in edge_index)]\n",
    "    edge_index = np.transpose(edge_index)\n",
    "    return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_save_graph(nodes_location, epsilon, sigma, dataset, distanceType, path_processed_data,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,path_distances,Y_data):\n",
    "\n",
    "        nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "        name_folder_weight = os.path.join(path_processed_data,'EdgeWeight')\n",
    "        name_folder_index = os.path.join(path_processed_data,'EdgeIndex')\n",
    "        if dataset in [Dataset.Experimental,Dataset.ExperimentalLR,Dataset.ExperimentalManual]:\n",
    "            dataset_name = Dataset.Experimental.name\n",
    "        if dataset in [Dataset.Tiny,Dataset.TinyLR,Dataset.TinyManual]:\n",
    "            dataset_name = Dataset.Tiny.name\n",
    "        array_distances = np.load(os.path.join(path_distances,f'distances_{distanceType.name}_{dataset_name}.npy'))\n",
    "\n",
    "        if not os.path.exists(name_folder_weight):\n",
    "            os.makedirs(name_folder_weight)\n",
    "\n",
    "        if not os.path.exists(name_folder_index):\n",
    "            os.makedirs(name_folder_index)\n",
    "\n",
    "        postfix = f'{distanceType.name}_{epsilon}_{sigma}_{dataset.name}'\n",
    "\n",
    "        name_weight = os.path.join(name_folder_weight,f'weight_{postfix}.npy')\n",
    "        name_index = os.path.join(name_folder_index,f'index_{postfix}.npy')\n",
    "\n",
    "        if os.path.exists(name_weight) and os.path.exists(name_index): \n",
    "            print(f'Graph already saved with configuration : epsilon = {epsilon}, sigma = {sigma}, size = {dataset.name}, distance = {distanceType.name}')\n",
    "            return\n",
    "\n",
    "        print(f'Saving graph with configuration : epsilon = {epsilon}, sigma = {sigma}, size = {dataset.name}, distance = {distanceType.name}')\n",
    "        if (dataset == Dataset.ExperimentalManual or dataset == Dataset.TinyManual):\n",
    "            edge_index, edge_weight = Graph_get_info_for_Manual(nodes_location, epsilon, sigma, dataset, distanceType,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,array_distances)\n",
    "        elif (dataset == Dataset.ExperimentalLR or dataset == Dataset.TinyLR):\n",
    "            edge_index, edge_weight = Graph_get_info_for_LR(nodes_location, epsilon, sigma, dataset, distanceType,array_distances,Y_data)\n",
    "        else:\n",
    "            edge_index, edge_weight = Graph_get_info_for_Standard(nodes_location, epsilon, sigma, dataset, distanceType,array_distances)\n",
    "        \n",
    "        np.save(name_index, edge_index)\n",
    "        np.save(name_weight, edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_ARRAY = [0.1, 0.3, 0.5, 0.7]\n",
    "SIGMA_ARRAY = [1, 3, 5, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in EPSILON_ARRAY:\n",
    "    for sigma in SIGMA_ARRAY:\n",
    "        for distanceType in DistanceType:\n",
    "            for dataset in Dataset:\n",
    "                Graph_save_graph(nodes_location, epsilon, sigma, dataset, distanceType, path_processed_data,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,path_distances,Y_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGraphMatrixFromGraph(edge_index, edge_weight, dataset):\n",
    "    nr_nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "    graph_matrix = np.zeros((nr_nodes, nr_nodes))\n",
    "    for index, (edge_index, edge_weight) in enumerate(\n",
    "            zip(np.transpose(edge_index), edge_weight)):\n",
    "        graph_matrix[edge_index[0]][edge_index[1]] = edge_weight\n",
    "    return graph_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphHeatmap(epsilon,sigma,distanceType,dataset,proccessed_data_path):\n",
    "    name_index = os.path.join(proccessed_data_path, \"EdgeIndex\", \n",
    "        f\"index_{distanceType.name}_{epsilon}_{sigma}_{dataset.name}\")\n",
    "    name_weight = os.path.join(proccessed_data_path, \"EdgeWeight\", \n",
    "        f\"weight_{distanceType.name}_{epsilon}_{sigma}_{dataset.name}\")\n",
    "    edge_index = np.load(name_index)\n",
    "    edge_weight = np.load(name_weight)\n",
    "    graph_matrix = GetGraphMatrixFromGraph(edge_index, edge_weight, dataset)\n",
    "    fig = px.imshow(\n",
    "        graph_matrix,\n",
    "        title='Graph Heatmap for epsilon {0} and sigma {1} with size {2}'\n",
    "        .format(epsilon, sigma, dataset.name))\n",
    "    \n",
    "    iplot(fig, filename=f'GraphHeatmap_{dataset.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphHeatmap(0.1,1,DistanceType.OSRM,Dataset.Experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(data, num_nodes):\n",
    "    r\"\"\"\n",
    "        Function to arrange data. At a point it represents the temporal \n",
    "        state of the graph\n",
    "    \"\"\"\n",
    "    New_Data = []\n",
    "    for i in range((int)(len(data) / num_nodes)):\n",
    "        Data = []\n",
    "        for k in range(num_nodes):\n",
    "            Data.append(data[(i * num_nodes) + k])\n",
    "        New_Data.append(Data)\n",
    "    return New_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_by_nodes(dataset,nodes_location,X,Y):\n",
    "    r\"\"\"\n",
    "        Returns data for a specific datasize.\n",
    "        Instance Function.\n",
    "        Args:\n",
    "            size : DatasetSize\n",
    "        Returns a tuple of 2 lists with the Data and Labels\n",
    "    \"\"\"\n",
    "\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    nodes_index = 0\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    for _, tuple in enumerate(zip(X, Y)):\n",
    "        if int(nodes_location[nodes_index][0]) in nodes_ids:\n",
    "            new_X.append(tuple[0])\n",
    "            new_Y.append([tuple[1]])\n",
    "        nodes_index += 1\n",
    "        if nodes_index == len(nodes_location):\n",
    "            nodes_index = 0\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_for_train_LSTM(dataset,path_processed_data,nodes_location, X_data, Y_data):\n",
    "    print(f\"Saving data with configuration : size = {dataset.name}\")\n",
    "\n",
    "    X_all, Y_all = get_clean_data_by_nodes(dataset,nodes_location, X_data, Y_data)\n",
    "\n",
    "    num_nodes = Graph_get_number_of_nodes_for_dataset(dataset)\n",
    "\n",
    "    X_all = arrange_data(X_all, num_nodes)\n",
    "    Y_all = arrange_data(Y_all, num_nodes)\n",
    "\n",
    "    proccessed_data_path_model = os.path.join(path_processed_data,\"LSTM\")\n",
    "    if not os.path.exists(proccessed_data_path_model):\n",
    "        os.makedirs(proccessed_data_path_model)\n",
    "\n",
    "    name_folder = os.path.join(proccessed_data_path_model,f'Data_{dataset.name}')\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.makedirs(name_folder)\n",
    "\n",
    "    for index, data in enumerate(X_all):\n",
    "        name_x = os.path.join(name_folder, f'X_{index}.npy')\n",
    "        if not os.path.exists(name_x):\n",
    "            np.save(name_x, data)\n",
    "\n",
    "    for index, data in enumerate(Y_all):\n",
    "        name_y = os.path.join(name_folder, f'Y_{index}.npy')\n",
    "        if not os.path.exists(name_y):\n",
    "            np.save(name_y, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_proccess_data_STCONV(dataset,nb_days,path_processed_data,nodes_location, X_data, Y_data):\n",
    "    batch_size = 8\n",
    "    time_steps = 1\n",
    "    interval_per_day = (int)(24 * 60 / 5)\n",
    "\n",
    "    print(f\"Saving data with configuration : size = {dataset.name}\")\n",
    "\n",
    "    Skip = (int)(time_steps / 2) * 2\n",
    "    interval_per_day -= Skip\n",
    "\n",
    "    X_all, Y_all = get_clean_data_by_nodes(dataset,nodes_location, X_data, Y_data)\n",
    "    num_nodes = Graph_get_number_of_nodes_for_dataset(dataset)\n",
    "\n",
    "    data_size_old = len(X_all)\n",
    "\n",
    "    X_all = arrange_data(X_all, num_nodes)\n",
    "    Y_all = arrange_data(Y_all, num_nodes)\n",
    "\n",
    "    new_size = (int)(interval_per_day * nb_days / batch_size)\n",
    "    data_size = len(X_all)\n",
    "    if new_size * batch_size != data_size:\n",
    "        difference = data_size - ((new_size - 1) * batch_size)\n",
    "        X_all = X_all[:data_size - difference]\n",
    "        Y_all = Y_all[:data_size - difference]\n",
    "        new_size -= 1\n",
    "    X_all = np.array(X_all).reshape(new_size, batch_size, time_steps, num_nodes,\n",
    "                            2)\n",
    "    Y_all = np.array(Y_all).reshape(new_size, batch_size, time_steps, num_nodes,\n",
    "                            1)\n",
    "\n",
    "    proccessed_data_path_model = os.path.join(path_processed_data,\"STCONV\")\n",
    "    if not os.path.exists(proccessed_data_path_model):\n",
    "        os.makedirs(proccessed_data_path_model)\n",
    "\n",
    "    name_folder = os.path.join(proccessed_data_path_model,f'Data_{dataset.name}')\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.makedirs(name_folder)\n",
    "\n",
    "    for index, data in enumerate(X_all):\n",
    "        name_x = os.path.join(name_folder, f'X_{index}.npy')\n",
    "        if not os.path.exists(name_x):\n",
    "            np.save(name_x, data)\n",
    "\n",
    "    for index, data in enumerate(Y_all):\n",
    "        name_y = os.path.join(name_folder, f'Y_{index}.npy')\n",
    "        if not os.path.exists(name_y):\n",
    "            np.save(name_y, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [Dataset.Experimental,Dataset.Tiny]:\n",
    "    save_proccess_data_STCONV(dataset,nb_days,path_processed_data,nodes_location,X_data,Y_data)\n",
    "    save_data_for_train_LSTM(dataset,path_processed_data,nodes_location,X_data,Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumeration used for the folders name in which the processed data will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldersProcesedNames(Enum):\n",
    "    STCONV = 0\n",
    "    LSTM = 1\n",
    "    EdgeWeight = 2\n",
    "    EdgeIndex = 3\n",
    "    Distances = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which will return true if the folders created after data processing exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Datareader_is_data_read(path_processed_data : str,folders_procesed_names : FoldersProcesedNames):\n",
    "    for folder_name in folders_procesed_names:\n",
    "        is_read_folder = os.path.isdir(os.path.join(path_processed_data, str(folder_name.name)))\n",
    "        if not is_read_folder:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data is read and stored: {Datareader_is_data_read(path_processed_data,FoldersProcesedNames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    r\"\"\"\n",
    "        Enumeration for each model type.\n",
    "            LSTM = 0\n",
    "            STCONV = 1\n",
    "            LinearRegression = 2\n",
    "    \"\"\"\n",
    "    LSTM = 0\n",
    "    DCRNN = 1\n",
    "    STCONV = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossFunction():\n",
    "\n",
    "    def Criterions():\n",
    "        return [\n",
    "            LossFunction.RMSE, LossFunction.MAPE, LossFunction.MAE,\n",
    "            LossFunction.MSE\n",
    "        ]\n",
    "\n",
    "    def RMSE(y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
    "\n",
    "    def MAPE(y_pred, y_true):\n",
    "        return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "    def MAE(y_pred, y_true):\n",
    "        return torch.mean(torch.abs((y_true - y_pred)))\n",
    "\n",
    "    def MSE(y_pred, y_true):\n",
    "        return torch.mean((y_true - y_pred)**2)\n",
    "\n",
    "    #endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STConvModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, num_nodes, hidden_channels, kernel_size,\n",
    "                 ):\n",
    "        super(STConvModel, self).__init__()\n",
    "\n",
    "        self.STCONV = STConv(num_nodes=num_nodes,\n",
    "                             in_channels=node_features,\n",
    "                             hidden_channels=hidden_channels,\n",
    "                             out_channels=node_features,\n",
    "                             kernel_size=kernel_size,\n",
    "                             K=1)\n",
    "        self.linear = Linear(node_features, 1)\n",
    "        self.ReLU = ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DCRNNModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, hidden_channels):\n",
    "        super(DCRNNModel, self).__init__()\n",
    "\n",
    "        self.DCRNN = DCRNN(in_channels=node_features,\n",
    "                           out_channels=hidden_channels,\n",
    "                           K=1)\n",
    "        self.Conv1 = GCNConv(in_channels=hidden_channels,\n",
    "                             out_channels=hidden_channels)\n",
    "        self.BatchNorm1 = BatchNorm1d(num_features=hidden_channels)\n",
    "        self.ReLU = ReLU()\n",
    "        self.Dropout = Dropout()\n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.DCRNN(x, edge_index, edge_weight)\n",
    "        x = self.Conv1(x, edge_index)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, hidden_channels):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.GCLSTM1 = GCLSTM(in_channels=node_features,\n",
    "                              out_channels=hidden_channels,\n",
    "                              K=1)\n",
    "        self.Conv1 = GCNConv(in_channels=hidden_channels,\n",
    "                             out_channels=hidden_channels)\n",
    "        self.BatchNorm1 = BatchNorm1d(num_features=hidden_channels)\n",
    "        self.ReLU = ReLU()\n",
    "        self.Dropout = Dropout()\n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.GCLSTM1(x, edge_index, edge_weight)\n",
    "        x = self.Conv1(x[0], edge_index)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHelper():\n",
    "    def __init__(self,\n",
    "                 sigma: int,\n",
    "                 epsilon: float,\n",
    "                 size: DatasetSize,\n",
    "                 distanceType: DistanceType,\n",
    "                 proccessed_data_path,\n",
    "                 folder_name,\n",
    "                 device: str = 'cpu',\n",
    "                 time_start: int = 0,\n",
    "                 time_stop: float = -1):\n",
    "                 \n",
    "        self.proccessed_data_path = proccessed_data_path\n",
    "        self.sigma = sigma\n",
    "        self.epsilon = epsilon\n",
    "        self.time_start = time_start\n",
    "        self.time_stop = time_stop\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.folder_name = folder_name\n",
    "        self.distanceType = distanceType\n",
    "        self.proccessed_data_path_model = os.path.join(self.proccessed_data_path, self.folder_name.name)\n",
    "        self.__set_graph()\n",
    "        self.__check_temporal_consistency()\n",
    "        self.__set_snapshot_count()\n",
    "\n",
    "    def __set_graph(self):\n",
    "        name_weight = os.path.join(\n",
    "            self.proccessed_data_path, 'EdgeWeight',\n",
    "            f'weight_{self.distanceType.name}_{self.epsilon}_{self.sigma}_{self.dataset.name}.npy')\n",
    "        self.edge_weight = np.load(name_weight, allow_pickle=True)\n",
    "\n",
    "        name_index = os.path.join(\n",
    "            self.proccessed_data_path, 'EdgeIndex',\n",
    "            f'index_{self.distanceType.name}_{self.epsilon}_{self.sigma}_{self.dataset.name}.npy')\n",
    "        self.edge_index = np.load(name_index, allow_pickle=True)\n",
    "\n",
    "    def get_edge_index(self):\n",
    "        if self.edge_index is None:\n",
    "            return self.edge_index\n",
    "        else:\n",
    "            return torch.LongTensor(self.edge_index).to(self.device)\n",
    "\n",
    "    def get_edge_weight(self):\n",
    "        if self.edge_weight is None:\n",
    "            return self.edge_weight\n",
    "        else:\n",
    "            return torch.FloatTensor(self.edge_weight).to(self.device)\n",
    "\n",
    "    def __get_features(self, time_index: int):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        name_x = os.path.join(self.proccessed_data_path_model,\n",
    "                              f\"Data_{dataset.name}\", f'X_{time_index}.npy')\n",
    "        X = np.load(name_x)\n",
    "        if X is None:\n",
    "            return X\n",
    "        else:\n",
    "            return torch.FloatTensor(X).to(self.device)\n",
    "\n",
    "    def __get_target(self, time_index: int):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        name_y = os.path.join(self.proccessed_data_path_model,\n",
    "                              f\"Data_{dataset.name}\", f'Y_{time_index}.npy')\n",
    "        Y = np.load(name_y)\n",
    "        if Y is None:\n",
    "            return Y\n",
    "        else:\n",
    "            if Y.dtype.kind == 'i':\n",
    "                return torch.LongTensor(Y).to(self.device)\n",
    "            elif Y.dtype.kind == 'f':\n",
    "                return torch.FloatTensor(Y).to(self.device)\n",
    "\n",
    "    def __getitem__(self, time_index: int):\n",
    "        x = self.__get_features(time_index)\n",
    "        y = self.__get_target(time_index)\n",
    "        return x, y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.t < self.time_stop:\n",
    "            snapshot = self.__getitem__(self.t)\n",
    "            self.t = self.t + 1\n",
    "            return snapshot\n",
    "        else:\n",
    "            self.t = self.time_start\n",
    "            raise StopIteration\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.t = self.time_start\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.snapshot_count\n",
    "\n",
    "    def __set_snapshot_count(self):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        self.snapshot_count = len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"X_*.npy\")))\n",
    "    \n",
    "    def __check_temporal_consistency(self):\n",
    "        assert len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"X_*.npy\"))) == len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"Y_*.npy\"))), \"Temporal dimension inconsistency.\"\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        train_ratio = 0.6\n",
    "        val_ratio= 0.2\n",
    "        test_ratio= 0.2\n",
    "\n",
    "        time_train = int(train_ratio * self.snapshot_count)\n",
    "        time_test = time_train + int(test_ratio * self.snapshot_count)\n",
    "\n",
    "        train_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                       self.distanceType, self.proccessed_data_path, self.folder_name,\n",
    "                                       self.device, 0, time_train)\n",
    "\n",
    "        test_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                      self.distanceType,  self.proccessed_data_path, self.folder_name,\n",
    "                                      self.device, time_train + 1, time_test)\n",
    "\n",
    "        val_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                     self.distanceType,  self.proccessed_data_path, self.folder_name,\n",
    "                                     self.device, time_test + 1,\n",
    "                                     self.snapshot_count)\n",
    "\n",
    "        return train_iterator, test_iterator, val_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(best_model, dfResults, epoch, test_dataset,model_type) -> None:\n",
    "    best_model.eval()\n",
    "    loss = 0\n",
    "    dataloader = DataLoader(test_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = test_dataset.get_edge_index()\n",
    "    edge_weight = test_dataset.get_edge_weight()\n",
    "    MAE_loss = 0\n",
    "    for criterion in LossFunction.Criterions():\n",
    "        loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = best_model(X, edge_index, edge_weight)\n",
    "            loss += criterion(y_hat, Y)\n",
    "            if criterion == LossFunction.MAE:\n",
    "                MAE_loss += criterion(y_hat, Y)\n",
    "\n",
    "        loss = loss / (index + 1)\n",
    "        loss = loss.item()\n",
    "\n",
    "        results = {\n",
    "            \"Model\": str(model_type.name),\n",
    "            \"Epsilon\": str(epsilon),\n",
    "            \"Sigma\": str(sigma),\n",
    "            \"Dataset\": str(dataset.name),\n",
    "            \"Criterion\": str(criterion.__name__),\n",
    "            \"Loss\": str(loss),\n",
    "            \"Epoch\": str(epoch),\n",
    "            \"TestOrVal\": \"Test\",\n",
    "            \"Trial\": tune.get_trial_id()\n",
    "        }\n",
    "        dfResults = dfResults.append(results, ignore_index=True)\n",
    "\n",
    "        if criterion == LossFunction.MAE:\n",
    "            MAE_loss = MAE_loss / (index + 1)\n",
    "            MAE_loss = MAE_loss.item()\n",
    "\n",
    "    print(\"Best trial test set loss: {}\".format(MAE_loss))\n",
    "    return dfResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dfResults, epoch,validation_dataset,model,model_type,dataset):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    dataloader = DataLoader(validation_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = validation_dataset.get_edge_index()\n",
    "    edge_weight = validation_dataset.get_edge_weight()\n",
    "    MAE_loss = 0\n",
    "    for criterion in LossFunction.Criterions():\n",
    "        loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = model(X, edge_index, edge_weight)\n",
    "            loss += criterion(y_hat, Y)\n",
    "            if criterion == LossFunction.MAE:\n",
    "                MAE_loss += criterion(y_hat, Y)\n",
    "\n",
    "        loss = loss / (index + 1)\n",
    "        loss = loss.item()\n",
    "\n",
    "        results = {\n",
    "            \"Model\": str(model_type.name),\n",
    "            \"Epsilon\": str(epsilon),\n",
    "            \"Sigma\": str(sigma),\n",
    "            \"Dataset\": str(dataset.name),\n",
    "            \"Criterion\": str(criterion.__name__),\n",
    "            \"Loss\": str(loss),\n",
    "            \"Epoch\": str(epoch),\n",
    "            \"TestOrVal\": \"Validation\",\n",
    "            \"Trial\": tune.get_trial_id()\n",
    "        }\n",
    "        dfResults = dfResults.append(results, ignore_index=True)\n",
    "\n",
    "        if criterion == LossFunction.MAE:\n",
    "            MAE_loss = MAE_loss / (index + 1)\n",
    "            MAE_loss = MAE_loss.item()\n",
    "\n",
    "    return MAE_loss, dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_and_test(experiment_name,model,model_type,optimizer,scheduler,\n",
    "    EarlyStoppingPatience,train_dataset,validation_dataset,test_dataset,nb_epoch,results_path):\n",
    "    dfResults = pd.DataFrame(columns=[\n",
    "        \"Model\", \"Epsilon\", \"Sigma\", \"Dataset\", \"Criterion\", \"Loss\", \"Epoch\", \"Trial\", \"TestOrVal\"\n",
    "    ])\n",
    "    train_model = model\n",
    "    train_model.train()\n",
    "    best_val_loss = np.inf\n",
    "    val_model = model\n",
    "    epoch_no_improvement = EarlyStoppingPatience\n",
    "    dataloader = DataLoader(train_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = train_dataset.get_edge_index()\n",
    "    edge_weight = train_dataset.get_edge_weight()\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        train_loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = train_model(X, edge_index, edge_weight)\n",
    "            loss = LossFunction.MAE(y_hat, Y)\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation Step at epoch end\n",
    "        val_loss, dfResults = validate(dfResults, epoch,validation_dataset,model,model_type,dataset)\n",
    "        if val_loss < best_val_loss:\n",
    "            val_model = copy.deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "            epoch_no_improvement = EarlyStoppingPatience\n",
    "        else:\n",
    "            epoch_no_improvement -= 1\n",
    "\n",
    "        if epoch_no_improvement == 0:\n",
    "            print(\"Early stopping at epoch: {0}\".format(epoch))\n",
    "            dfResults = test(val_model, dfResults, epoch, test_dataset,model_type)\n",
    "            if not os.path.exists(\n",
    "                    os.path.join(results_path, experiment_name)):\n",
    "                os.makedirs(\n",
    "                    os.path.join(results_path, experiment_name))\n",
    "            file_save = os.path.join( results_path, experiment_name,\n",
    "                f\"{model_type.name}_{dataset.name}_{distanceType.name}_{tune.get_trial_id()}.csv\")\n",
    "            dfResults.to_csv(path_or_buf=file_save, index=False)\n",
    "            break\n",
    "\n",
    "        train_loss = train_loss / (index + 1)\n",
    "        scheduler.step(train_loss)\n",
    "        print(\"Epoch {0} : Validation loss {1} ; Train loss {2};\".format(\n",
    "            epoch, val_loss, train_loss))\n",
    "\n",
    "        # test step if this is the last epoch\n",
    "        # Save dataframe results\n",
    "        if epoch == nb_epoch - 1:\n",
    "            dfResults = test(val_model, dfResults, epoch, test_dataset,model_type)\n",
    "            if not os.path.exists(\n",
    "                    os.path.join(results_path, experiment_name)):\n",
    "                os.makedirs(\n",
    "                    os.path.join(results_path, experiment_name))\n",
    "            file_save = os.path.join(\n",
    "                results_path, experiment_name,\n",
    "                f\"{model_type.name}_{dataset.name}_{distanceType.name}_{tune.get_trial_id()}.csv\")\n",
    "            dfResults.to_csv(path_or_buf=file_save, index=False)\n",
    "\n",
    "        # Log to tune\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save(\n",
    "                (val_model.state_dict(), optimizer.state_dict()),\n",
    "                path)\n",
    "\n",
    "        tune.report(loss=val_loss)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train(config, param, checkpoint_dir=None) -> None:\n",
    "    folder_name = config[\"modelType\"]\n",
    "    if config[\"modelType\"] == ModelType.DCRNN:\n",
    "        folder_name = ModelType.LSTM\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = DatasetHelper(\n",
    "                config[\"sigma\"],\n",
    "                config[\"epsilon\"],\n",
    "                config[\"dataset\"],\n",
    "                param[\"distanceType\"],\n",
    "                param[\"proccessed_data_path\"],\n",
    "                folder_name).get_dataset()\n",
    "    if config[\"modelType\"] == ModelType.STCONV:\n",
    "        model = STConvModel(node_features=2,\n",
    "                                     num_nodes=Graph_get_number_of_nodes_for_dataset(config[\"dataset\"]),\n",
    "                                     hidden_channels=32,\n",
    "                                     kernel_size=1)\n",
    "    elif config[\"modelType\"] == ModelType.LSTM:\n",
    "        model = LSTMModel(node_features=2, hidden_channels=32)\n",
    "    elif config[\"modelType\"] == ModelType.DCRNN:\n",
    "        model = DCRNNModel(node_features=2, hidden_channels=32)\n",
    "    optimizer = Adam(model.parameters(), lr=param[\"learning_rate\"])\n",
    "    scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                           mode='min',\n",
    "                                           factor=0.1,\n",
    "                                           patience=5,\n",
    "                                           threshold=0.00000001,\n",
    "                                           threshold_mode='abs')\n",
    "    train_validate_and_test(param[\"experiment_name\"],model,config[\"modelType\"],optimizer,scheduler,\n",
    "    param[\"EarlyStoppingPatience\"],train_dataset,validation_dataset,test_dataset,param[\"nb_epoch\"],param[\"results_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trail_dirname_creator(trial):\n",
    "        return f\"{trial.config['modelType'].name}_{trial.config['dataset'].name}_{datetime.now().strftime('%d_%m_%Y-%H_%M_%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperParameterTuning(dataset, model, distanceType, experiment_name,\n",
    "    results_ray_path,epsilon_array,sigma_array,proccessed_data_path,results_path):\n",
    "\n",
    "    nb_epoch = 30\n",
    "    grace_period = 30\n",
    "    reduction_factor = 3\n",
    "    scheduler = ASHAScheduler(max_t=nb_epoch,\n",
    "                                grace_period=grace_period,\n",
    "                                reduction_factor=reduction_factor)\n",
    "    learning_rate = 0.01\n",
    "    num_features = 2\n",
    "    EarlyStoppingPatience = 5\n",
    "    nb_epoch\n",
    "    param = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_features\": num_features,\n",
    "        \"EarlyStoppingPatience\": EarlyStoppingPatience,\n",
    "        \"nb_epoch\": nb_epoch,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"distanceType\": distanceType,\n",
    "        \"proccessed_data_path\": proccessed_data_path,\n",
    "        \"results_path\": results_path\n",
    "    }\n",
    "    \n",
    "    config = {\n",
    "        \"epsilon\": tune.choice(epsilon_array),\n",
    "        \"sigma\": tune.choice(sigma_array),\n",
    "        \"modelType\": tune.choice([model]),\n",
    "        \"dataset\" : tune.choice([dataset])\n",
    "    }\n",
    "\n",
    "\n",
    "    directory_experiment_ray = os.path.join(results_ray_path, experiment_name)\n",
    "\n",
    "    num_samples = 16\n",
    "    start_train.__name__ = f\"{model.name}_{dataset.name}_{distanceType.name}\"\n",
    "\n",
    "    result = tune.run(tune.with_parameters(start_train, param=param),\n",
    "                        local_dir=directory_experiment_ray,\n",
    "                        trial_dirname_creator=trail_dirname_creator,\n",
    "                        resources_per_trial={\n",
    "                            \"cpu\": 8,\n",
    "                            \"gpu\": 1\n",
    "                        },\n",
    "                        config=config,\n",
    "                        metric=\"loss\",\n",
    "                        mode=\"min\",\n",
    "                        num_samples=num_samples,\n",
    "                        scheduler=scheduler,\n",
    "                        verbose=0)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial for {} model final validation loss: {}\".format(\n",
    "        model.name, best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(current_directory,\"Results\")\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "results_ray_path = os.path.join(current_directory,\"Results-RAY\")\n",
    "if not os.path.exists(results_ray_path):\n",
    "    os.makedirs(results_ray_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_learn(results_ray_path,path_processed_data,results_path):\n",
    "    experiment_name = f'Experiment_{datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\")}'\n",
    "    directory_experiment_ray = os.path.join(results_ray_path, experiment_name)\n",
    "    if not os.path.exists(directory_experiment_ray):\n",
    "        os.makedirs(directory_experiment_ray)\n",
    "        \n",
    "    for distanceType in DistanceType:\n",
    "        for dataset in [Dataset.Experimental,Dataset.Tiny]:\n",
    "            for model in [ModelType.STCONV]:\n",
    "                hyperParameterTuning(dataset, model, distanceType, experiment_name,\n",
    "                    results_ray_path,EPSILON_ARRAY,SIGMA_ARRAY,path_processed_data,results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learn(results_ray_path,path_processed_data,results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_INFO = [\n",
    "        \"Model\", \"Epsilon\", \"Sigma\", \"Size\", \"Criterion\", \"Loss\", \"Epoch\",\n",
    "        \"OptimizerType\", \"Trial\", \"TestOrVal\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOneFile(experiment_path, columnsInfo, type):\n",
    "    File = os.path.join(experiment_path, f\"{type}.csv\")\n",
    "    if not (os.path.exists(File)):\n",
    "        dataframeInfo = pd.DataFrame(columns=columnsInfo)\n",
    "        Files = os.path.join(experiment_path, f\"{type}*_*.csv\")\n",
    "        for file in glob(Files):\n",
    "            distanceType = Path(file).stem.split(\"_\")[2]\n",
    "            with open(file) as f:\n",
    "                dataframeRead = pd.read_csv(\n",
    "                    file,\n",
    "                    sep=',',\n",
    "                    header=None,\n",
    "                    names=columnsInfo,\n",
    "                    skiprows=1)\n",
    "                dataframeRead[\"DistanceType\"] = distanceType\n",
    "                dataframeInfo = dataframeInfo.append(dataframeRead,ignore_index=True)\n",
    "\n",
    "        dataframeInfo.to_csv(File)\n",
    "    return pd.read_csv(os.path.join(experiment_path, f\"{type}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(experiment_path):\n",
    "    dfLR = pd.read_csv(\n",
    "        os.path.join(experiment_path, \"LinearRegression.csv\"))\n",
    "    dfARIMA = pd.read_csv(\n",
    "        os.path.join(experiment_path, \"ARIMA.csv\"))\n",
    "    dfSARIMA = pd.read_csv(\n",
    "        os.path.join(experiment_path, \"SARIMA.csv\"))\n",
    "    dfSTCONV = makeOneFile(experiment_path,COLUMNS_INFO, \"STCONV\")\n",
    "    dfLSTM = makeOneFile(experiment_path, COLUMNS_INFO,\n",
    "                                    \"LSTM\")\n",
    "    dfDCRNN = makeOneFile(experiment_path,\n",
    "                                        COLUMNS_INFO, \"DCRNN\")\n",
    "    return dfLR,dfARIMA,dfSARIMA, dfSTCONV, dfLSTM, dfDCRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = [d for d in os.listdir(results_ray_path) if os.path.isdir(d)]\n",
    "latest_experiment = max(all_experiments, key=os.path.getmtime)\n",
    "experiment_path = os.path.join(results_path, latest_experiment)\n",
    "dfSTCONV, dfLSTM, dfDCRNN = read_results(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_table_height(df, base=208, height_per_row=20, char_limit=30, height_padding=16.5):\n",
    "    '''\n",
    "    df: The dataframe with only the columns you want to plot\n",
    "    base: The base height of the table (header without any rows)\n",
    "    height_per_row: The height that one row requires\n",
    "    char_limit: If the length of a value crosses this limit, the row's height needs to be expanded to fit the value\n",
    "    height_padding: Extra height in a row when a length of value exceeds char_limit\n",
    "    '''\n",
    "    total_height = 0 + base\n",
    "    for x in range(df.shape[0]):\n",
    "        total_height += height_per_row\n",
    "    for y in range(df.shape[1]):\n",
    "        if len(str(df.iloc[x][y])) > char_limit:\n",
    "            total_height += height_padding\n",
    "    return total_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TableFinalResults(dfSTCONV,dfLSTM,dfDCRNN) -> None:\n",
    "\n",
    "    dfResults = dfSTCONV.append(dfDCRNN, ignore_index=True)\n",
    "    dfResults = dfResults.append(dfLSTM, ignore_index=True)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"Model\", \"Size\", \"Distance\", \"Generation\", \"RMSE\", \"MAPE\", \"MAE\", \"MSE\"])\n",
    "\n",
    "    \n",
    "    for model in [ModelType.DCRNN, ModelType.LSTM, ModelType.STCONV]:\n",
    "        for datasetsize in [Dataset.Experimental, Dataset.Tiny]:\n",
    "            for GenerationType in [\"Generative\", \"Manual\", \"LR\"]:\n",
    "                for distanceType in DistanceType:\n",
    "                    if not (GenerationType == \"Generative\"):\n",
    "                        datasetName = f\"{datasetsize.name}{GenerationType}\"\n",
    "                    else:\n",
    "                        datasetName = datasetsize.name\n",
    "                    if (model == ModelType.LSTM):\n",
    "                        modelName = \"GCLSTM\"\n",
    "                    else:\n",
    "                        modelName = model.name\n",
    "                    dfResultsTemp = dfResults[dfResults[\"Model\"]\n",
    "                                                == model.name]\n",
    "                    dfResultsTemp = dfResultsTemp[dfResultsTemp[\"Size\"]\n",
    "                                                    == datasetName]\n",
    "                    dfResultsTemp = dfResultsTemp[dfResultsTemp[\"DistanceType\"]\n",
    "                                                    == distanceType.name]\n",
    "                    dfResultsTemp = dfResultsTemp[[\"Criterion\", \"Loss\"]]\n",
    "                    dfResultsTemp = dfResultsTemp.groupby(\n",
    "                        [\"Criterion\"]).min().T\n",
    "                    df = df.append(\n",
    "                        {\n",
    "                            \"Model\": modelName,\n",
    "                            \"Size\": datasetsize.name,\n",
    "                            \"Distance\": distanceType.name,\n",
    "                            \"Generation\": GenerationType,\n",
    "                            \"RMSE\": format(\n",
    "                                (float)(dfResultsTemp[\"RMSE\"].iloc[0]), '.2f'),\n",
    "                            \"MAPE\": format(\n",
    "                                (float)(dfResultsTemp[\"MAPE\"].iloc[0]), '.2f'),\n",
    "                            \"MAE\": format((float)(dfResultsTemp[\"MAE\"].iloc[0]), '.2f'),\n",
    "                            \"MSE\": format((float)(dfResultsTemp[\"MSE\"].iloc[0]), '.2f')\n",
    "                        },\n",
    "                        ignore_index=True)\n",
    "\n",
    "    layout = dict(height=calc_table_height(df) + 300)\n",
    "    fig = go.Figure(data=[\n",
    "        go.Table(header=dict(values=list(df.columns),\n",
    "                                fill_color='paleturquoise',\n",
    "                                align='left'),\n",
    "                    cells=dict(values=[\n",
    "                        df.Model, df.Size, df.Distance, df.Generation, df.RMSE, df.MAPE, df.MAE, df.MSE\n",
    "                    ],\n",
    "            fill_color='lavender',\n",
    "            align='left'))\n",
    "    ], layout=layout)\n",
    "\n",
    "    iplot(fig, filename=f'TableFinalResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableFinalResults(dfSTCONV,dfLSTM,dfDCRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlotResults(dfSTCONV,dfLSTM,dfDCRNN,distanceType) -> None:\n",
    "    dfSTCONV[\"Type\"] = \"STCONV\"\n",
    "    dfLSTM[\"Type\"] = \"GCLSTM\"\n",
    "    dfDCRNN[\"Type\"] = \"DCRNN\"\n",
    "    df = dfSTCONV.append(dfLSTM, ignore_index=True)\n",
    "    df = df.append(dfDCRNN, ignore_index=True)\n",
    "    df = df[df[\"DistanceType\"] == distanceType.name]\n",
    "    df = df[df[\"Loss\"] < 20]\n",
    "    dfTemp = df[df[\"Criterion\"] == LossFunction.MAE.__name__]\n",
    "    fig = px.box(dfTemp, x=\"Size\", y=\"Loss\", color=\"Type\")\n",
    "    \n",
    "    iplot(fig, filename=f'BoxPlotResults')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxPlotResults(dfSTCONV,dfLSTM,dfDCRNN,DistanceType.OSRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxPlotResults(dfSTCONV,dfLSTM,dfDCRNN,DistanceType.Geodesic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SigmaEpsilonTable(dfSTCONV,dfDCRNN,dfLSTM) -> None:\n",
    "\n",
    "    dfResults = dfSTCONV.append(dfDCRNN, ignore_index=True)\n",
    "    dfResults = dfResults.append(dfLSTM, ignore_index=True)\n",
    "\n",
    "    df = pd.DataFrame(columns=SIGMA_ARRAY,\n",
    "                        index=EPSILON_ARRAY)\n",
    "    for sigma in SIGMA_ARRAY:\n",
    "        for epsilon in EPSILON_ARRAY:\n",
    "            dfResultsTemp = dfResults[dfResults[\"Sigma\"] == sigma]\n",
    "            dfResultsTemp = dfResultsTemp[dfResultsTemp[\"Epsilon\"] == epsilon]\n",
    "            dfResultsTemp = dfResultsTemp.groupby([\"Criterion\"]).min()\n",
    "            df.loc[epsilon][sigma] = format(\n",
    "                (float)(dfResultsTemp[\"Loss\"].iloc[0]), '.2f')\n",
    "    headers = list(df.columns)\n",
    "    headers.insert(0, \"\")\n",
    "    print(headers)\n",
    "    fig = go.Figure(data=[\n",
    "        go.Table(header=dict(values=headers,\n",
    "                                fill_color='paleturquoise',\n",
    "                                align='left'),\n",
    "                    cells=dict(values=[\n",
    "                        EPSILON_ARRAY, df[1], df[3], df[5], df[10]\n",
    "                    ],\n",
    "            fill_color='lavender',\n",
    "            align='left'))\n",
    "    ])\n",
    "\n",
    "    iplot(fig, filename=f'SigmaEpsilonTable')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Disertatie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6383c80193fdaf2fbe8b70c805160effbb4b92ae92c365f7e8068f4ed2143cb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
