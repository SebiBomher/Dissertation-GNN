{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing distances for graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from typing import Tuple\n",
    "from geopy.distance import geodesic\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import normalize\n",
    "from enum import Enum\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch_geometric_temporal.nn.attention.stgcn import STConv\n",
    "from torch_geometric_temporal.nn.recurrent import GCLSTM, DCRNN\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import ReLU, Linear, Module, BatchNorm1d, Dropout\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants needed for data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"Data\"\n",
    "GRAPH_INFO_TXT = \"d07_text_meta_2021_03_27.txt\"\n",
    "current_directory = os.getcwd()\n",
    "path_raw_data = os.path.join(current_directory, DATA_FOLDER)\n",
    "path_raw_data_graph_info_txt = os.path.join(path_raw_data, GRAPH_INFO_TXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set total number of nodes (nb_days) from Metadata (may contain empty nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_get_number_of_nodes(path_raw_data_graph_info_txt):\n",
    "    nodes_location = []\n",
    "    skip = True\n",
    "    with open(path_raw_data_graph_info_txt) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            if skip:\n",
    "                skip = False\n",
    "            else:\n",
    "                line = line.split('\\t')\n",
    "                line = line[:-1]         # ID     #LAT     #LONG\n",
    "                nodes_location.append([line[0], line[8], line[9]])\n",
    "\n",
    "    return len(nodes_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of nodes are : 4904\n"
     ]
    }
   ],
   "source": [
    "total_num_nodes = DataReader_get_number_of_nodes(path_raw_data_graph_info_txt)\n",
    "print(f'The total number of nodes are : {total_num_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nodes that have data as good_nodes and nodes that have empty data as empty_nodes from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_get_good_empty_nodes(path_raw_data,total_num_nodes):\n",
    "    index = total_num_nodes\n",
    "    empty_nodes = []\n",
    "    good_nodes = []\n",
    "    txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "    for file in glob(txtFiles):\n",
    "        with open(file) as f:\n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                line = line.split(',')\n",
    "                line = [line1.replace(\"\\n\", \"\") for line1 in line]\n",
    "                if not (line[9] == '' or line[10] == '' or line[11] == ''):\n",
    "                    good_nodes.append((int)(line[1]))\n",
    "                else:\n",
    "                    empty_nodes.append((int)(line[1]))\n",
    "                index -= 1\n",
    "                if index == 0:\n",
    "                    return (good_nodes,empty_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five examples of nodes that contain data: [715898, 715918, 715920, 715929, 715930]\n",
      "Five examples of nodes that do not have data: [715900, 715901, 715903, 715904, 715905]\n"
     ]
    }
   ],
   "source": [
    "(good_nodes, empty_nodes) = DataReader_get_good_empty_nodes(\n",
    "    path_raw_data, total_num_nodes)\n",
    "print(f'Five examples of nodes that contain data: {good_nodes[:5]}')\n",
    "print(f'Five examples of nodes that do not have data: {empty_nodes[:5]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data and labels as X and Y variables from Data. They will contain good nodes and empty nodes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_read_data(path_raw_data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "    nb_days = 0\n",
    "    for file in glob(txtFiles):\n",
    "        with open(file) as f:\n",
    "            print(f'Reading day {nb_days + 1}')\n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                line = line.split(',')\n",
    "                line = [line1.replace(\"\\n\", \"\") for line1 in line]\n",
    "                if not (line[9] == '' or line[10] == '' or line[11] == ''):\n",
    "                    Y.append((float)(line[11]))\n",
    "                    X.append([(float)(line[9]), (float)(line[10])])\n",
    "        nb_days += 1\n",
    "        # TODO : code for debugging, delete when finished\n",
    "        # ------------------------------------------------\n",
    "        # if nb_days == 2:\n",
    "        #     break\n",
    "        # ------------------------------------------------\n",
    "    X = normalize(np.array(X))\n",
    "    Y = Y\n",
    "    return X,Y,nb_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading day 1\n",
      "Reading day 2\n",
      "Reading day 3\n",
      "Reading day 4\n",
      "Reading day 5\n",
      "Reading day 6\n",
      "Reading day 7\n",
      "Reading day 8\n",
      "Reading day 9\n",
      "Reading day 10\n",
      "Reading day 11\n",
      "Reading day 12\n",
      "Reading day 13\n",
      "Reading day 14\n",
      "Reading day 15\n",
      "Reading day 16\n",
      "Reading day 17\n",
      "Reading day 18\n",
      "Reading day 19\n",
      "Reading day 20\n",
      "Reading day 21\n",
      "Reading day 22\n"
     ]
    }
   ],
   "source": [
    "X_data,Y_data,nb_days = DataReader_read_data(path_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days read: 22\n",
      "Five examples of normalized input data: [[9.99999984e-01 1.76732671e-04]\n",
      " [9.99999978e-01 2.08088231e-04]\n",
      " [9.99999974e-01 2.29906536e-04]\n",
      " [9.99999979e-01 2.03723400e-04]\n",
      " [9.99999976e-01 2.17045449e-04]]\n",
      "Five examples of input data label: [70.4, 69.2, 66.0, 72.5, 69.8]\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of days read: {nb_days}')\n",
    "print(f'Five examples of normalized input data: {X_data[:5]}')\n",
    "print(f'Five examples of input data label: {Y_data[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nodes geo location from Metadata. They will contain good nodes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataReader_read_nodes_data(path_raw_data_graph_info_txt, good_nodes):\n",
    "\n",
    "    nodes_location = []\n",
    "    skip = True\n",
    "    with open(path_raw_data_graph_info_txt) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            if skip:\n",
    "                skip = False\n",
    "            else:\n",
    "                line = line.split('\\t')\n",
    "                line = line[:-1]\n",
    "                if (int)(line[0]) in good_nodes:  # ID  #LAT    #LONG\n",
    "                    nodes_location.append([line[0], line[8], line[9]])\n",
    "    return nodes_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five examples of nodes geolocation as well as their ID's [['715898', '33.880183', '-118.021787'], ['715918', '33.93311', '-118.091005'], ['715920', '33.938544', '-118.094941'], ['715929', '33.971707', '-118.123095'], ['715930', '33.971763', '-118.122905']]\n"
     ]
    }
   ],
   "source": [
    "nodes_location = DataReader_read_nodes_data(\n",
    "    path_raw_data_graph_info_txt, good_nodes)\n",
    "print(\n",
    "    f'Five examples of nodes geolocation as well as their ID\\'s {nodes_location[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(path_raw_data,graph_info_txt):\n",
    "        columnsInfo = [\n",
    "            'Timestamp', 'Station', 'District', 'Freeway', 'DirOfTravel',\n",
    "            'LaneType', 'Length', 'Samples', 'Observed', 'Flow', 'Occupancy',\n",
    "            'Speed'\n",
    "        ]\n",
    "        for i in range(1, 9):\n",
    "            columnsInfo.extend([\n",
    "                str(i) + '_Samples',\n",
    "                str(i) + '_Flow',\n",
    "                str(i) + '_Occupancy',\n",
    "                str(i) + '_Speed',\n",
    "                str(i) + '_Observed'\n",
    "            ])\n",
    "        columnsMetadata = [\n",
    "            'ID', 'Fwy', 'Dir', 'District', 'County', 'City', 'State_PM',\n",
    "            'Abs_PM', 'Latitude', 'Longitude', 'Length', 'Type', 'Lanes',\n",
    "            'Name', 'User_ID_1', 'User_ID_2', 'User_ID_3', 'User_ID_4'\n",
    "        ]\n",
    "        txtFiles = os.path.join(path_raw_data, \"*\", \"*.txt\")\n",
    "        print(\"Reading Metadata\")\n",
    "        dataframeMetadata = pd.read_csv(path_raw_data_graph_info_txt,\n",
    "                                        sep='\\t',\n",
    "                                        skiprows=1,\n",
    "                                        header=None,\n",
    "                                        names=columnsMetadata)\n",
    "        print(\"Finished Reading Metadata\")\n",
    "        print(\"Reading Information\")\n",
    "        nb_days = 0\n",
    "        dataframeInfo = pd.DataFrame(columns=columnsInfo)\n",
    "\n",
    "        for file in glob(txtFiles):\n",
    "            print(\"Reading day {0}\".format(nb_days + 1))\n",
    "            with open(file) as f:\n",
    "                dataframeInfo = dataframeInfo.append(pd.read_csv(\n",
    "                    file, sep=',', header=None, names=columnsInfo),\n",
    "                                                     ignore_index=True)\n",
    "                day = None\n",
    "                nb_days += 1\n",
    "\n",
    "            if nb_days == 5:\n",
    "                break\n",
    "\n",
    "        print(\"Finished Reading Information\")\n",
    "        return dataframeInfo, dataframeMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Metadata\n",
      "Finished Reading Metadata\n",
      "Reading Information\n",
      "Reading day 1\n",
      "Reading day 2\n",
      "Reading day 3\n",
      "Reading day 4\n",
      "Reading day 5\n",
      "Finished Reading Information\n",
      "             Timestamp Station District Freeway DirOfTravel LaneType  Length  \\\n",
      "0  06/01/2021 00:00:00  715898        7       5           S       ML    0.43   \n",
      "1  06/01/2021 00:00:00  715900        7       5           S       OR     NaN   \n",
      "2  06/01/2021 00:00:00  715901        7       5           N       OR     NaN   \n",
      "3  06/01/2021 00:00:00  715903        7       5           N       OR     NaN   \n",
      "4  06/01/2021 00:00:00  715904        7       5           S       OR     NaN   \n",
      "\n",
      "  Samples Observed   Flow  ...  7_Samples  7_Flow  7_Occupancy  7_Speed  \\\n",
      "0       0        0  202.0  ...        NaN     NaN          NaN      NaN   \n",
      "1       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "2       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "3       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "4       0        0    NaN  ...        NaN     NaN          NaN      NaN   \n",
      "\n",
      "   7_Observed  8_Samples 8_Flow  8_Occupancy  8_Speed  8_Observed  \n",
      "0           0        NaN    NaN          NaN      NaN           0  \n",
      "1           0        NaN    NaN          NaN      NaN           0  \n",
      "2           0        NaN    NaN          NaN      NaN           0  \n",
      "3           0        NaN    NaN          NaN      NaN           0  \n",
      "4           0        NaN    NaN          NaN      NaN           0  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "       ID  Fwy Dir  District  County     City State_PM   Abs_PM   Latitude  \\\n",
      "0  715898    5   S         7      37  40032.0      .71  117.280  33.880183   \n",
      "1  715900    5   S         7      37  40032.0     1.06  117.630  33.882892   \n",
      "2  715901    5   N         7      37  40032.0     1.11  117.743  33.883400   \n",
      "3  715903    5   N         7      37  69154.0     1.56  118.193  33.886992   \n",
      "4  715904    5   S         7      37  69154.0     2.27  118.840  33.892489   \n",
      "\n",
      "    Longitude  Length Type  Lanes         Name  User_ID_1  User_ID_2  \\\n",
      "0 -118.021787    0.43   ML      3       PHOEBE       2029        NaN   \n",
      "1 -118.026822     NaN   OR      1  VALLEY VIEW       3255        NaN   \n",
      "2 -118.027451     NaN   OR      1  VALLEY VIEW       3268        NaN   \n",
      "3 -118.034125     NaN   OR      1      ALONDRA       3269        NaN   \n",
      "4 -118.044573     NaN   OR      1    CARMENITA       3253        NaN   \n",
      "\n",
      "   User_ID_3  User_ID_4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "path_plots = os.path.join(current_directory,\"Plots\")\n",
    "path_plots_general = os.path.join(path_plots,\"General\")\n",
    "dfInfo,dfMetaData = visualization(path_raw_data,path_raw_data_graph_info_txt)\n",
    "print(dfInfo[:5])\n",
    "print(dfMetaData[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxPlotSpeed(dfInfo) -> None:\n",
    "\n",
    "    df = dfInfo\n",
    "    df = df[df['Speed'].notna()]\n",
    "    df['Day'] = df['Timestamp'].apply(lambda x: datetime.strptime(\n",
    "        x, '%m/%d/%Y %H:%M:%S').weekday())\n",
    "    fig = px.box(df, x='Day', y='Speed')\n",
    "    iplot(fig, filename='BoxPlotSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxPlotSpeed(dfInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumartion used for Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Enum):\n",
    "    Experimental = 0\n",
    "    ExperimentalManual = 1\n",
    "    ExperimentalLR = 2\n",
    "    Tiny = 3\n",
    "    TinyManual = 4\n",
    "    TinyLR = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNodes(Enum):\n",
    "    Experimental = [ 718292, 769496, 718291, 718290, 764567, 774279, 774278, 764671 ]\n",
    "    Tiny = [\n",
    "        775637, 718165, 776986, 759289, 774672, 760643, 774671, 717046, 718419,\n",
    "        769105, 764026, 759280, 775636, 759385, 760635, 718166, 774685, 774658,\n",
    "        716938, 776177, 763453, 718421, 717045, 768598, 717043, 716063, 717041,\n",
    "        717040, 717039, 737184, 717042, 718335, 763458, 776981, 737158, 737313,\n",
    "        769118, 772501, 718173, 764037, 763447, 763246, 718041, 763251, 763424,\n",
    "        763429, 763434, 763439, 764032, 764418\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSize(Enum):\n",
    "    Experimental = len(DatasetNodes.Experimental.value)\n",
    "    Tiny = len(DatasetNodes.Tiny.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to set the nodes for the datasets such that they are the same throughout the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More constants defined, as well as the nodes id's for the Experimental and Tiny dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_nodes_for_dataset(dataset):\n",
    "    experimental_all = [Dataset.Experimental,\n",
    "                        Dataset.ExperimentalManual, Dataset.ExperimentalLR]\n",
    "    tiny_all = [Dataset.Tiny, Dataset.TinyManual, Dataset.TinyLR]\n",
    "    if dataset in experimental_all:\n",
    "        return DatasetNodes.Experimental.value\n",
    "    elif dataset in tiny_all:\n",
    "        return DatasetNodes.Tiny.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718292, 769496, 718291, 718290, 764567, 774279, 774278, 764671]\n"
     ]
    }
   ],
   "source": [
    "print(Graph_get_nodes_for_dataset(Dataset.Experimental))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_number_of_nodes_for_dataset(dataset):\n",
    "    experimental_all = [Dataset.Experimental,\n",
    "                        Dataset.ExperimentalManual, Dataset.ExperimentalLR]\n",
    "    tiny_all = [Dataset.Tiny, Dataset.TinyManual, Dataset.TinyLR]\n",
    "    if dataset in experimental_all:\n",
    "        return DatasetSize.Experimental.value\n",
    "    elif dataset in tiny_all:\n",
    "        return DatasetSize.Tiny.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(Graph_get_number_of_nodes_for_dataset(Dataset.Experimental))\n",
    "print(Graph_get_number_of_nodes_for_dataset(Dataset.Tiny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_extract_time(json):\n",
    "    try:\n",
    "        return float(json['routes']['distance'])\n",
    "    except KeyError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_OSRM_loop(p1: tuple, p2: tuple) -> float:\n",
    "    requestUrl = f'http://router.project-osrm.org/route/v1/driving/{p1[1]},{p1[0]};{p2[1]},{p2[0]}'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(requestUrl)\n",
    "    except:\n",
    "        return -1\n",
    "    if (response.status_code != 204\n",
    "            and response.headers[\"content-type\"].strip().startswith(\n",
    "                \"application/json\")):\n",
    "        try:\n",
    "            responseJson = response.json()\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    if responseJson['code'] == \"Ok\":\n",
    "        routes = responseJson['routes']\n",
    "        routes.sort(key=Graph_extract_time, reverse=True)\n",
    "        shortest_distance = float(routes[0]['distance']) * (1 / 1000)\n",
    "        return shortest_distance\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_OSRM(p1: tuple, p2: tuple) -> float:\n",
    "    distance = Graph_OSRM_loop(p1, p2)\n",
    "    while (distance == -1):\n",
    "        distance = Graph_OSRM_loop(p1, p2)\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geodesic distance between node id 715898 and node id 715918 is 8.68599891018443 km\n",
      "Road distance between node id 715898 and node id 715918 is 8.849 km\n"
     ]
    }
   ],
   "source": [
    "p1 = ('33.880183', '-118.021787')\n",
    "p2 = ('33.93311', '-118.091005')\n",
    "print(\n",
    "    f'Geodesic distance between node id 715898 and node id 715918 is {geodesic(p1,p2)}')\n",
    "print(\n",
    "    f'Road distance between node id 715898 and node id 715918 is {Graph_OSRM(p1,p2)} km')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_compute_all_OSRM_and_Geodesic(nodes_location, path_distances):\n",
    "    for dataset in [Dataset.Experimental, Dataset.Tiny]:\n",
    "\n",
    "        name_OSRM = os.path.join(\n",
    "            path_distances, f'distances_OSRM_{dataset.name}.npy')\n",
    "        name_geodesic = os.path.join(\n",
    "            path_distances, f'distances_Geodesic_{dataset.name}.npy')\n",
    "\n",
    "        if (os.path.exists(name_OSRM) and os.path.exists(name_geodesic)):\n",
    "            print(f'Distances already computed for {dataset.name}')\n",
    "            continue\n",
    "\n",
    "        nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "        nodes_location_dataset = [node for node in nodes_location if (int)(node[0]) in nodes_ids]\n",
    "\n",
    "        matrix_size = len(nodes_location_dataset)\n",
    "\n",
    "        OSRM_array = np.zeros((matrix_size, matrix_size))\n",
    "        geodesic_array = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        for i in range(matrix_size - 1):\n",
    "            for j in range(i + 1, matrix_size):\n",
    "                p1 = ((float)(nodes_location_dataset[i][1]),\n",
    "                      (float)(nodes_location_dataset[i][2]))\n",
    "                p2 = ((float)(nodes_location_dataset[j][1]),\n",
    "                      (float)(nodes_location_dataset[j][2]))\n",
    "\n",
    "                id_1 = (int)(nodes_location_dataset[i][0])\n",
    "                id_2 = (int)(nodes_location_dataset[j][0])\n",
    "\n",
    "                print(f'Computing distances for Id\\'s {id_1} and {id_2}')\n",
    "\n",
    "                OSRM_array[i][j] = Graph_OSRM(p1, p2)\n",
    "                geodesic_array[i][j] = geodesic(p1, p2).km\n",
    "                OSRM_array[j][i] = Graph_OSRM(p2, p1)\n",
    "                geodesic_array[j][i] = geodesic(p2, p1).km\n",
    "\n",
    "        np.save(name_OSRM, OSRM_array)\n",
    "        np.save(name_geodesic, geodesic_array)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_processed_data = os.path.join(current_directory, \"Processed\")\n",
    "if not os.path.exists(path_processed_data):\n",
    "    os.makedirs(path_processed_data)\n",
    "\n",
    "path_distances = os.path.join(path_processed_data, \"Distances\")\n",
    "if not os.path.exists(path_distances):\n",
    "    os.makedirs(path_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started computing distances...\n",
      "Distances already computed for Experimental\n",
      "Distances already computed for Tiny\n",
      "Finished computing distances...\n"
     ]
    }
   ],
   "source": [
    "print(\"Started computing distances...\")\n",
    "Graph_compute_all_OSRM_and_Geodesic(nodes_location, path_distances)\n",
    "print(\"Finished computing distances...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     2.3406 3.5436 7.6358 7.1201 3.2997 3.6629 5.1336]\n",
      " [1.8412 0.     1.203  5.2952 4.7795 0.9591 1.3223 3.7805]\n",
      " [4.7693 7.1099 0.     4.0921 3.5765 8.069  6.1248 5.4476]\n",
      " [0.6772 3.0178 4.2208 0.     7.7973 3.9769 4.3401 5.8108]\n",
      " [1.1929 3.5335 0.7643 0.5157 0.     4.4926 2.5484 3.1371]\n",
      " [5.0133 7.3539 0.2439 4.3361 3.8204 0.     6.3688 5.6916]\n",
      " [3.573  5.9136 3.4803 5.0002 4.4845 6.8727 0.     2.4172]\n",
      " [3.1712 5.5117 2.5385 6.6306 6.1149 6.4709 3.2839 0.    ]]\n",
      "[[0.         0.02477349 1.20259522 0.67630258 1.19130412 0.95912002\n",
      "  1.15459812 1.1201794 ]\n",
      " [0.02477349 0.         1.20137815 0.67577782 1.19058306 0.95783532\n",
      "  1.15965723 1.12529727]\n",
      " [1.20259522 1.20137815 0.         0.52659971 0.02639492 0.24354287\n",
      "  0.30303407 0.30835885]\n",
      " [0.67630258 0.67577782 0.52659971 0.         0.51500154 0.28354955\n",
      "  0.52112531 0.48918423]\n",
      " [1.19130412 1.19058306 0.02639492 0.51500154 0.         0.23375099\n",
      "  0.27704067 0.28200812]\n",
      " [0.95912002 0.95783532 0.24354287 0.28354955 0.23375099 0.\n",
      "  0.33559618 0.31512439]\n",
      " [1.15459812 1.15965723 0.30303407 0.52112531 0.27704067 0.33559618\n",
      "  0.         0.03450204]\n",
      " [1.1201794  1.12529727 0.30835885 0.48918423 0.28200812 0.31512439\n",
      "  0.03450204 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "array_distances_OSRM_Experimental = np.load(os.path.join(\n",
    "    path_distances, f'distances_OSRM_Experimental.npy'))\n",
    "array_distances_Geodesic_Experimental = np.load(os.path.join(\n",
    "    path_distances, f'distances_Geodesic_Experimental.npy'))\n",
    "print(array_distances_OSRM_Experimental)\n",
    "print(array_distances_Geodesic_Experimental)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceType(Enum):\n",
    "    Geodesic = 0\n",
    "    OSRM = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_adjency_matrix_weight(ID1_index, ID2_index, epsilon, sigma, distanceType,distances_array) -> float:\n",
    "    distance = distances_array[ID1_index][ID2_index]\n",
    "    weight = math.exp(-((distance**2) / (sigma**2)))\n",
    "    if weight >= epsilon:\n",
    "        return weight\n",
    "    else:\n",
    "        return 0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_Experimental_manual = [[0, 1, 7, 4, 7, 5], [1, 2, 4, 3, 6, 4]]\n",
    "edge_index_Tiny_manual = [[\n",
    "    0, 0, 0, 1, 5, 5, 5, 9, 9, 9, 10, 10, 10, 6, 14, 15, 7, 13, 10, 11, 8,\n",
    "    9, 4, 16, 5, 2, 20, 3, 22, 23, 24, 25, 26, 28, 29, 30, 31, 21, 32, 33,\n",
    "    34, 36, 17, 38, 12, 39, 40, 41, 42, 44, 45, 46, 47, 48, 27, 35, 9\n",
    "],\n",
    "    [\n",
    "    1, 2, 3, 4, 6, 7, 2, 4, 3, 12, 12, 11,\n",
    "    4, 14, 15, 12, 13, 10, 11, 8, 2, 3, 16,\n",
    "    17, 6, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "    29, 30, 31, 7, 32, 33, 34, 35, 9, 37,\n",
    "    5, 39, 40, 41, 42, 43, 45, 46, 47, 48,\n",
    "    0, 19, 18, 36\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_Standard(nodes_location, epsilon, sigma, dataset, distanceType,array_distances):\n",
    "    nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    nodes_location_manual = [node for node in nodes_location if (int)(node[0]) in nodes]\n",
    "    num_nodes = len(nodes_location_manual)\n",
    "    for i in range(num_nodes - 1):\n",
    "        for j in range(i, num_nodes - 1):\n",
    "            if i != j:\n",
    "                weight = Graph_get_adjency_matrix_weight(i,j,epsilon, sigma, distanceType,array_distances)\n",
    "                if weight > 0:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(weight)\n",
    "    edge_index = np.transpose(edge_index)\n",
    "    return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_Manual(nodes_location, epsilon, sigma, dataset, distanceType,\n",
    "            edge_index_Experimental_manual,edge_index_Tiny_manual,array_distances):\n",
    "    \n",
    "    edge_weight = []\n",
    "    if dataset == Dataset.ExperimentalManual:\n",
    "        edge_index = edge_index_Experimental_manual\n",
    "    elif dataset == Dataset.TinyManual:\n",
    "        edge_index = edge_index_Tiny_manual\n",
    "    nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "    nodes_location_standard = [node for node in nodes_location if (int)(node[0]) in nodes]\n",
    "    num_nodes = len(nodes_location_standard)\n",
    "    nodes_info = np.zeros((num_nodes, 3))\n",
    "    for i in range(num_nodes):\n",
    "        np.where(nodes == (int)(nodes_location_standard[i][0]))[0]\n",
    "        nodes_info[np.where(nodes == (int)(nodes_location_standard[i][0]))[0]] = nodes_location_standard[i]\n",
    "    nodes_location_standard = np.array(nodes_info)\n",
    "    for i in range(len(edge_index[0])):\n",
    "        weight = Graph_get_adjency_matrix_weight(\n",
    "            edge_index[0][i],edge_index[1][i], epsilon, sigma, distanceType,array_distances\n",
    "        )\n",
    "        edge_weight.append(weight)\n",
    "    return edge_index, edge_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_top_3_nodes_for_node_with_LR(node, dataset, nodes_location, speed_vector):\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    ids_index = 0\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    nodes_used = []\n",
    "    node_ids_order = [(int)(node[0]) for node in nodes_location]\n",
    "    nodes_used_computed = False\n",
    "    X_train_snapshot = []\n",
    "    for speed in speed_vector:\n",
    "\n",
    "        if node != node_ids_order[ids_index] and node_ids_order[ids_index] in nodes_ids:\n",
    "            X_train_snapshot.append(speed)\n",
    "            if not nodes_used_computed:\n",
    "                nodes_used.append(node_ids_order[ids_index])\n",
    "\n",
    "        if node == node_ids_order[ids_index]:\n",
    "            Y_train.append(speed)\n",
    "\n",
    "        ids_index += 1\n",
    "        if ids_index == len(node_ids_order): \n",
    "            ids_index = 0\n",
    "            X_train.append(X_train_snapshot)\n",
    "            X_train_snapshot = []\n",
    "            nodes_used_computed = True\n",
    "\n",
    "    regression = LinearRegression(positive=True).fit(X_train, Y_train)\n",
    "    coeffiecients = regression.coef_.tolist()\n",
    "    results = zip(nodes_used, coeffiecients)\n",
    "    sorted_results = sorted(results, key=lambda tup: tup[1], reverse=True)\n",
    "    best = sorted_results[:3]\n",
    "    return [result[0] for result in best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[759385, 718335, 764032]\n"
     ]
    }
   ],
   "source": [
    "print(Graph_get_top_3_nodes_for_node_with_LR(717042, Dataset.TinyLR, nodes_location, Y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_get_info_for_LR(nodes_location, epsilon, sigma, dataset, distanceType,array_distances, Y_data):\n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    for node in nodes_ids:\n",
    "        nodes_relevant = Graph_get_top_3_nodes_for_node_with_LR(node, dataset, nodes_location, Y_data)\n",
    "        for node_relevant in nodes_relevant:\n",
    "            edge_index.append([nodes_ids.index(node_relevant),nodes_ids.index(node)])\n",
    "            edge_weight.append(Graph_get_adjency_matrix_weight(\n",
    "                    nodes_ids.index(node_relevant), nodes_ids.index(node),\n",
    "                    epsilon, sigma, distanceType,array_distances))\n",
    "    edge_index = [list(x) for x in set(tuple(x) for x in edge_index)]\n",
    "    edge_index = np.transpose(edge_index)\n",
    "    return edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_save_graph(nodes_location, epsilon, sigma, dataset, distanceType, path_processed_data,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,path_distances,Y_data):\n",
    "\n",
    "        nodes = Graph_get_nodes_for_dataset(dataset)\n",
    "        name_folder_weight = os.path.join(path_processed_data,'EdgeWeight')\n",
    "        name_folder_index = os.path.join(path_processed_data,'EdgeIndex')\n",
    "        if dataset in [Dataset.Experimental,Dataset.ExperimentalLR,Dataset.ExperimentalManual]:\n",
    "            dataset_name = Dataset.Experimental.name\n",
    "        if dataset in [Dataset.Tiny,Dataset.TinyLR,Dataset.TinyManual]:\n",
    "            dataset_name = Dataset.Tiny.name\n",
    "        array_distances = np.load(os.path.join(path_distances,f'distances_{distanceType.name}_{dataset_name}.npy'))\n",
    "\n",
    "        if not os.path.exists(name_folder_weight):\n",
    "            os.makedirs(name_folder_weight)\n",
    "\n",
    "        if not os.path.exists(name_folder_index):\n",
    "            os.makedirs(name_folder_index)\n",
    "\n",
    "        postfix = f'{distanceType.name}_{epsilon}_{sigma}_{dataset.name}'\n",
    "\n",
    "        name_weight = os.path.join(name_folder_weight,f'weight_{postfix}.npy')\n",
    "        name_index = os.path.join(name_folder_index,f'index_{postfix}.npy')\n",
    "\n",
    "        if os.path.exists(name_weight) and os.path.exists(name_index): \n",
    "            print(f'Graph already saved with configuration : epsilon = {epsilon}, sigma = {sigma}, size = {dataset.name}, distance = {distanceType.name}')\n",
    "            return\n",
    "\n",
    "        print(f'Saving graph with configuration : epsilon = {epsilon}, sigma = {sigma}, size = {dataset.name}, distance = {distanceType.name}')\n",
    "        if (dataset == Dataset.ExperimentalManual or dataset == Dataset.TinyManual):\n",
    "            edge_index, edge_weight = Graph_get_info_for_Manual(nodes_location, epsilon, sigma, dataset, distanceType,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,array_distances)\n",
    "        elif (dataset == Dataset.ExperimentalLR or dataset == Dataset.TinyLR):\n",
    "            edge_index, edge_weight = Graph_get_info_for_LR(nodes_location, epsilon, sigma, dataset, distanceType,array_distances,Y_data)\n",
    "        else:\n",
    "            edge_index, edge_weight = Graph_get_info_for_Standard(nodes_location, epsilon, sigma, dataset, distanceType,array_distances)\n",
    "        \n",
    "        np.save(name_index, edge_index)\n",
    "        np.save(name_weight, edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_ARRAY = [0.1, 0.3, 0.5, 0.7]\n",
    "SIGMA_ARRAY = [1, 3, 5, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 1, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 3, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 5, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.1, sigma = 10, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 1, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 3, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 5, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.3, sigma = 10, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 1, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 3, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 5, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.5, sigma = 10, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 1, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 3, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 5, size = TinyLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = Experimental, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = ExperimentalManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = ExperimentalLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = Tiny, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = TinyManual, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = TinyLR, distance = Geodesic\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = Experimental, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = ExperimentalManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = ExperimentalLR, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = Tiny, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = TinyManual, distance = OSRM\n",
      "Graph already saved with configuration : epsilon = 0.7, sigma = 10, size = TinyLR, distance = OSRM\n"
     ]
    }
   ],
   "source": [
    "for epsilon in EPSILON_ARRAY:\n",
    "    for sigma in SIGMA_ARRAY:\n",
    "        for distanceType in DistanceType:\n",
    "            for dataset in Dataset:\n",
    "                Graph_save_graph(nodes_location, epsilon, sigma, dataset, distanceType, path_processed_data,\n",
    "                    edge_index_Experimental_manual,edge_index_Tiny_manual,path_distances,Y_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(data, num_nodes):\n",
    "    r\"\"\"\n",
    "        Function to arrange data. At a point it represents the temporal \n",
    "        state of the graph\n",
    "    \"\"\"\n",
    "    New_Data = []\n",
    "    for i in range((int)(len(data) / num_nodes)):\n",
    "        Data = []\n",
    "        for k in range(num_nodes):\n",
    "            Data.append(data[(i * num_nodes) + k])\n",
    "        New_Data.append(Data)\n",
    "    return New_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_by_nodes(dataset,nodes_location,X,Y):\n",
    "    r\"\"\"\n",
    "        Returns data for a specific datasize.\n",
    "        Instance Function.\n",
    "        Args:\n",
    "            size : DatasetSize\n",
    "        Returns a tuple of 2 lists with the Data and Labels\n",
    "    \"\"\"\n",
    "\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    nodes_index = 0\n",
    "    nodes_ids = Graph_get_nodes_for_dataset(dataset)\n",
    "    for _, tuple in enumerate(zip(X, Y)):\n",
    "        if int(nodes_location[nodes_index][0]) in nodes_ids:\n",
    "            new_X.append(tuple[0])\n",
    "            new_Y.append([tuple[1]])\n",
    "        nodes_index += 1\n",
    "        if nodes_index == len(nodes_location):\n",
    "            nodes_index = 0\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_for_train_LSTM(dataset,path_processed_data,nodes_location, X_data, Y_data):\n",
    "    print(f\"Saving data with configuration : size = {dataset.name}\")\n",
    "\n",
    "    X_all, Y_all = get_clean_data_by_nodes(dataset,nodes_location, X_data, Y_data)\n",
    "\n",
    "    num_nodes = Graph_get_number_of_nodes_for_dataset(dataset)\n",
    "\n",
    "    X_all = arrange_data(X_all, num_nodes)\n",
    "    Y_all = arrange_data(Y_all, num_nodes)\n",
    "\n",
    "    proccessed_data_path_model = os.path.join(path_processed_data,\"LSTM\")\n",
    "    if not os.path.exists(proccessed_data_path_model):\n",
    "        os.makedirs(proccessed_data_path_model)\n",
    "\n",
    "    name_folder = os.path.join(proccessed_data_path_model,f'Data_{dataset.name}')\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.makedirs(name_folder)\n",
    "\n",
    "    for index, data in enumerate(X_all):\n",
    "        name_x = os.path.join(name_folder, f'X_{index}.npy')\n",
    "        if not os.path.exists(name_x):\n",
    "            np.save(name_x, data)\n",
    "\n",
    "    for index, data in enumerate(Y_all):\n",
    "        name_y = os.path.join(name_folder, f'Y_{index}.npy')\n",
    "        if not os.path.exists(name_y):\n",
    "            np.save(name_y, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_proccess_data_STCONV(dataset,nb_days,path_processed_data,nodes_location, X_data, Y_data):\n",
    "    batch_size = 8\n",
    "    time_steps = 1\n",
    "    interval_per_day = (int)(24 * 60 / 5)\n",
    "\n",
    "    print(f\"Saving data with configuration : size = {dataset.name}\")\n",
    "\n",
    "    Skip = (int)(time_steps / 2) * 2\n",
    "    interval_per_day -= Skip\n",
    "\n",
    "    X_all, Y_all = get_clean_data_by_nodes(dataset,nodes_location, X_data, Y_data)\n",
    "    num_nodes = Graph_get_number_of_nodes_for_dataset(dataset)\n",
    "\n",
    "    data_size_old = len(X_all)\n",
    "\n",
    "    X_all = arrange_data(X_all, num_nodes)\n",
    "    Y_all = arrange_data(Y_all, num_nodes)\n",
    "\n",
    "    new_size = (int)(interval_per_day * nb_days / batch_size)\n",
    "    data_size = len(X_all)\n",
    "    if new_size * batch_size != data_size:\n",
    "        difference = data_size - ((new_size - 1) * batch_size)\n",
    "        X_all = X_all[:data_size - difference]\n",
    "        Y_all = Y_all[:data_size - difference]\n",
    "        new_size -= 1\n",
    "    X_all = np.array(X_all).reshape(new_size, batch_size, time_steps, num_nodes,\n",
    "                            2)\n",
    "    Y_all = np.array(Y_all).reshape(new_size, batch_size, time_steps, num_nodes,\n",
    "                            1)\n",
    "\n",
    "    proccessed_data_path_model = os.path.join(path_processed_data,\"STCONV\")\n",
    "    if not os.path.exists(proccessed_data_path_model):\n",
    "        os.makedirs(proccessed_data_path_model)\n",
    "\n",
    "    name_folder = os.path.join(proccessed_data_path_model,f'Data_{dataset.name}')\n",
    "    if not os.path.exists(name_folder):\n",
    "        os.makedirs(name_folder)\n",
    "\n",
    "    for index, data in enumerate(X_all):\n",
    "        name_x = os.path.join(name_folder, f'X_{index}.npy')\n",
    "        if not os.path.exists(name_x):\n",
    "            np.save(name_x, data)\n",
    "\n",
    "    for index, data in enumerate(Y_all):\n",
    "        name_y = os.path.join(name_folder, f'Y_{index}.npy')\n",
    "        if not os.path.exists(name_y):\n",
    "            np.save(name_y, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data with configuration : size = Experimental\n",
      "Saving data with configuration : size = Experimental\n",
      "Saving data with configuration : size = Tiny\n",
      "Saving data with configuration : size = Tiny\n"
     ]
    }
   ],
   "source": [
    "for dataset in [Dataset.Experimental,Dataset.Tiny]:\n",
    "    save_proccess_data_STCONV(dataset,nb_days,path_processed_data,nodes_location,X_data,Y_data)\n",
    "    save_data_for_train_LSTM(dataset,path_processed_data,nodes_location,X_data,Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumeration used for the folders name in which the processed data will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldersProcesedNames(Enum):\n",
    "    STCONV = 0\n",
    "    LSTM = 1\n",
    "    EdgeWeight = 2\n",
    "    EdgeIndex = 3\n",
    "    Distances = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which will return true if the folders created after data processing exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Datareader_is_data_read(path_processed_data : str,folders_procesed_names : FoldersProcesedNames):\n",
    "    for folder_name in folders_procesed_names:\n",
    "        is_read_folder = os.path.isdir(os.path.join(path_processed_data, str(folder_name.name)))\n",
    "        if not is_read_folder:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is read and stored: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Data is read and stored: {Datareader_is_data_read(path_processed_data,FoldersProcesedNames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    r\"\"\"\n",
    "        Enumeration for each model type.\n",
    "            LSTM = 0\n",
    "            STCONV = 1\n",
    "            LinearRegression = 2\n",
    "    \"\"\"\n",
    "    LSTM = 0\n",
    "    DCRNN = 1\n",
    "    STCONV = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossFunction():\n",
    "\n",
    "    def Criterions():\n",
    "        return [\n",
    "            LossFunction.RMSE, LossFunction.MAPE, LossFunction.MAE,\n",
    "            LossFunction.MSE\n",
    "        ]\n",
    "\n",
    "    def RMSE(y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
    "\n",
    "    def MAPE(y_pred, y_true):\n",
    "        return torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "    def MAE(y_pred, y_true):\n",
    "        return torch.mean(torch.abs((y_true - y_pred)))\n",
    "\n",
    "    def MSE(y_pred, y_true):\n",
    "        return torch.mean((y_true - y_pred)**2)\n",
    "\n",
    "    #endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STConvModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, num_nodes, hidden_channels, kernel_size,\n",
    "                 ):\n",
    "        super(STConvModel, self).__init__()\n",
    "\n",
    "        self.STCONV = STConv(num_nodes=num_nodes,\n",
    "                             in_channels=node_features,\n",
    "                             hidden_channels=hidden_channels,\n",
    "                             out_channels=node_features,\n",
    "                             kernel_size=kernel_size,\n",
    "                             K=1)\n",
    "        self.linear = Linear(node_features, 1)\n",
    "        self.ReLU = ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.STCONV(x, edge_index, edge_weight)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DCRNNModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, hidden_channels):\n",
    "        super(DCRNNModel, self).__init__()\n",
    "\n",
    "        self.DCRNN = DCRNN(in_channels=node_features,\n",
    "                           out_channels=hidden_channels,\n",
    "                           K=1)\n",
    "        self.Conv1 = GCNConv(in_channels=hidden_channels,\n",
    "                             out_channels=hidden_channels)\n",
    "        self.BatchNorm1 = BatchNorm1d(num_features=hidden_channels)\n",
    "        self.ReLU = ReLU()\n",
    "        self.Dropout = Dropout()\n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.DCRNN(x, edge_index, edge_weight)\n",
    "        x = self.Conv1(x, edge_index)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(Module):\n",
    "\n",
    "    def __init__(self, node_features, hidden_channels):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.GCLSTM1 = GCLSTM(in_channels=node_features,\n",
    "                              out_channels=hidden_channels,\n",
    "                              K=1)\n",
    "        self.Conv1 = GCNConv(in_channels=hidden_channels,\n",
    "                             out_channels=hidden_channels)\n",
    "        self.BatchNorm1 = BatchNorm1d(num_features=hidden_channels)\n",
    "        self.ReLU = ReLU()\n",
    "        self.Dropout = Dropout()\n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.GCLSTM1(x, edge_index, edge_weight)\n",
    "        x = self.Conv1(x[0], edge_index)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHelper():\n",
    "    def __init__(self,\n",
    "                 sigma: int,\n",
    "                 epsilon: float,\n",
    "                 size: DatasetSize,\n",
    "                 distanceType: DistanceType,\n",
    "                 proccessed_data_path,\n",
    "                 folder_name,\n",
    "                 device: str = 'cpu',\n",
    "                 time_start: int = 0,\n",
    "                 time_stop: float = -1):\n",
    "                 \n",
    "        self.proccessed_data_path = proccessed_data_path\n",
    "        self.sigma = sigma\n",
    "        self.epsilon = epsilon\n",
    "        self.time_start = time_start\n",
    "        self.time_stop = time_stop\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.folder_name = folder_name\n",
    "        self.distanceType = distanceType\n",
    "        self.proccessed_data_path_model = os.path.join(self.proccessed_data_path, self.folder_name.name)\n",
    "        self.__set_graph()\n",
    "        self.__check_temporal_consistency()\n",
    "        self.__set_snapshot_count()\n",
    "\n",
    "    def __set_graph(self):\n",
    "        name_weight = os.path.join(\n",
    "            self.proccessed_data_path, 'EdgeWeight',\n",
    "            f'weight_{self.distanceType.name}_{self.epsilon}_{self.sigma}_{self.dataset.name}.npy')\n",
    "        self.edge_weight = np.load(name_weight, allow_pickle=True)\n",
    "\n",
    "        name_index = os.path.join(\n",
    "            self.proccessed_data_path, 'EdgeIndex',\n",
    "            f'index_{self.distanceType.name}_{self.epsilon}_{self.sigma}_{self.dataset.name}.npy')\n",
    "        self.edge_index = np.load(name_index, allow_pickle=True)\n",
    "\n",
    "    def get_edge_index(self):\n",
    "        if self.edge_index is None:\n",
    "            return self.edge_index\n",
    "        else:\n",
    "            return torch.LongTensor(self.edge_index).to(self.device)\n",
    "\n",
    "    def get_edge_weight(self):\n",
    "        if self.edge_weight is None:\n",
    "            return self.edge_weight\n",
    "        else:\n",
    "            return torch.FloatTensor(self.edge_weight).to(self.device)\n",
    "\n",
    "    def __get_features(self, time_index: int):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        name_x = os.path.join(self.proccessed_data_path_model,\n",
    "                              f\"Data_{dataset.name}\", f'X_{time_index}.npy')\n",
    "        X = np.load(name_x)\n",
    "        if X is None:\n",
    "            return X\n",
    "        else:\n",
    "            return torch.FloatTensor(X).to(self.device)\n",
    "\n",
    "    def __get_target(self, time_index: int):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        name_y = os.path.join(self.proccessed_data_path_model,\n",
    "                              f\"Data_{dataset.name}\", f'Y_{time_index}.npy')\n",
    "        Y = np.load(name_y)\n",
    "        if Y is None:\n",
    "            return Y\n",
    "        else:\n",
    "            if Y.dtype.kind == 'i':\n",
    "                return torch.LongTensor(Y).to(self.device)\n",
    "            elif Y.dtype.kind == 'f':\n",
    "                return torch.FloatTensor(Y).to(self.device)\n",
    "\n",
    "    def __getitem__(self, time_index: int):\n",
    "        x = self.__get_features(time_index)\n",
    "        y = self.__get_target(time_index)\n",
    "        return x, y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.t < self.time_stop:\n",
    "            snapshot = self.__getitem__(self.t)\n",
    "            self.t = self.t + 1\n",
    "            return snapshot\n",
    "        else:\n",
    "            self.t = self.time_start\n",
    "            raise StopIteration\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.t = self.time_start\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.snapshot_count\n",
    "\n",
    "    def __set_snapshot_count(self):\n",
    "        if (self.dataset in [Dataset.TinyManual, Dataset.TinyLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        elif (self.dataset in [Dataset.ExperimentalManual, Dataset.ExperimentalLR]):\n",
    "            dataset = DatasetSize.Tiny\n",
    "        else:\n",
    "            dataset = self.dataset\n",
    "        self.snapshot_count = len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"X_*.npy\")))\n",
    "    \n",
    "    def __check_temporal_consistency(self):\n",
    "        assert len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"X_*.npy\"))) == len(\n",
    "            glob( os.path.join(self.proccessed_data_path_model, f\"Data_{dataset.name}\", \"Y_*.npy\"))), \"Temporal dimension inconsistency.\"\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        train_ratio = 0.6\n",
    "        val_ratio= 0.2\n",
    "        test_ratio= 0.2\n",
    "\n",
    "        time_train = int(train_ratio * self.snapshot_count)\n",
    "        time_test = time_train + int(test_ratio * self.snapshot_count)\n",
    "\n",
    "        train_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                       self.distanceType, self.proccessed_data_path, self.folder_name,\n",
    "                                       self.device, 0, time_train)\n",
    "\n",
    "        test_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                      self.distanceType,  self.proccessed_data_path, self.folder_name,\n",
    "                                      self.device, time_train + 1, time_test)\n",
    "\n",
    "        val_iterator = DatasetHelper(self.sigma, self.epsilon, self.dataset,\n",
    "                                     self.distanceType,  self.proccessed_data_path, self.folder_name,\n",
    "                                     self.device, time_test + 1,\n",
    "                                     self.snapshot_count)\n",
    "\n",
    "        return train_iterator, test_iterator, val_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(best_model, dfResults, epoch, test_dataset,model_type) -> None:\n",
    "    best_model.eval()\n",
    "    loss = 0\n",
    "    dataloader = DataLoader(test_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = test_dataset.get_edge_index()\n",
    "    edge_weight = test_dataset.get_edge_weight()\n",
    "    MAE_loss = 0\n",
    "    for criterion in LossFunction.Criterions():\n",
    "        loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = best_model(X, edge_index, edge_weight)\n",
    "            loss += criterion(y_hat, Y)\n",
    "            if criterion == LossFunction.MAE:\n",
    "                MAE_loss += criterion(y_hat, Y)\n",
    "\n",
    "        loss = loss / (index + 1)\n",
    "        loss = loss.item()\n",
    "\n",
    "        results = {\n",
    "            \"Model\": str(model_type.name),\n",
    "            \"Epsilon\": str(epsilon),\n",
    "            \"Sigma\": str(sigma),\n",
    "            \"Dataset\": str(dataset.name),\n",
    "            \"Criterion\": str(criterion.__name__),\n",
    "            \"Loss\": str(loss),\n",
    "            \"Epoch\": str(epoch),\n",
    "            \"TestOrVal\": \"Test\",\n",
    "            \"Trial\": tune.get_trial_id()\n",
    "        }\n",
    "        dfResults = dfResults.append(results, ignore_index=True)\n",
    "\n",
    "        if criterion == LossFunction.MAE:\n",
    "            MAE_loss = MAE_loss / (index + 1)\n",
    "            MAE_loss = MAE_loss.item()\n",
    "\n",
    "    print(\"Best trial test set loss: {}\".format(MAE_loss))\n",
    "    return dfResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dfResults, epoch,validation_dataset,model,model_type,dataset):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    dataloader = DataLoader(validation_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = validation_dataset.get_edge_index()\n",
    "    edge_weight = validation_dataset.get_edge_weight()\n",
    "    MAE_loss = 0\n",
    "    for criterion in LossFunction.Criterions():\n",
    "        loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = model(X, edge_index, edge_weight)\n",
    "            loss += criterion(y_hat, Y)\n",
    "            if criterion == LossFunction.MAE:\n",
    "                MAE_loss += criterion(y_hat, Y)\n",
    "\n",
    "        loss = loss / (index + 1)\n",
    "        loss = loss.item()\n",
    "\n",
    "        results = {\n",
    "            \"Model\": str(model_type.name),\n",
    "            \"Epsilon\": str(epsilon),\n",
    "            \"Sigma\": str(sigma),\n",
    "            \"Dataset\": str(dataset.name),\n",
    "            \"Criterion\": str(criterion.__name__),\n",
    "            \"Loss\": str(loss),\n",
    "            \"Epoch\": str(epoch),\n",
    "            \"TestOrVal\": \"Validation\",\n",
    "            \"Trial\": tune.get_trial_id()\n",
    "        }\n",
    "        dfResults = dfResults.append(results, ignore_index=True)\n",
    "\n",
    "        if criterion == LossFunction.MAE:\n",
    "            MAE_loss = MAE_loss / (index + 1)\n",
    "            MAE_loss = MAE_loss.item()\n",
    "\n",
    "    return MAE_loss, dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_and_test(experiment_name,model,model_type,optimizer,scheduler,\n",
    "    EarlyStoppingPatience,train_dataset,validation_dataset,test_dataset,nb_epoch,results_path):\n",
    "    dfResults = pd.DataFrame(columns=[\n",
    "        \"Model\", \"Epsilon\", \"Sigma\", \"Dataset\", \"Criterion\", \"Loss\", \"Epoch\", \"Trial\", \"TestOrVal\"\n",
    "    ])\n",
    "    train_model = model\n",
    "    train_model.train()\n",
    "    best_val_loss = np.inf\n",
    "    val_model = model\n",
    "    epoch_no_improvement = EarlyStoppingPatience\n",
    "    dataloader = DataLoader(train_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    edge_index = train_dataset.get_edge_index()\n",
    "    edge_weight = train_dataset.get_edge_weight()\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        train_loss = 0\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            X = x[0]\n",
    "            Y = y[0]\n",
    "            y_hat = train_model(X, edge_index, edge_weight)\n",
    "            loss = LossFunction.MAE(y_hat, Y)\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation Step at epoch end\n",
    "        val_loss, dfResults = validate(dfResults, epoch,validation_dataset,model,model_type,dataset)\n",
    "        if val_loss < best_val_loss:\n",
    "            val_model = copy.deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "            epoch_no_improvement = EarlyStoppingPatience\n",
    "        else:\n",
    "            epoch_no_improvement -= 1\n",
    "\n",
    "        if epoch_no_improvement == 0:\n",
    "            print(\"Early stopping at epoch: {0}\".format(epoch))\n",
    "            dfResults = test(val_model, dfResults, epoch, test_dataset,model_type)\n",
    "            if not os.path.exists(\n",
    "                    os.path.join(results_path, experiment_name)):\n",
    "                os.makedirs(\n",
    "                    os.path.join(results_path, experiment_name))\n",
    "            file_save = os.path.join( results_path, experiment_name,\n",
    "                f\"{model_type.name}_{dataset.name}_{distanceType.name}_{tune.get_trial_id()}.csv\")\n",
    "            dfResults.to_csv(path_or_buf=file_save, index=False)\n",
    "            break\n",
    "\n",
    "        train_loss = train_loss / (index + 1)\n",
    "        scheduler.step(train_loss)\n",
    "        print(\"Epoch {0} : Validation loss {1} ; Train loss {2};\".format(\n",
    "            epoch, val_loss, train_loss))\n",
    "\n",
    "        # test step if this is the last epoch\n",
    "        # Save dataframe results\n",
    "        if epoch == nb_epoch - 1:\n",
    "            dfResults = test(val_model, dfResults, epoch, test_dataset,model_type)\n",
    "            if not os.path.exists(\n",
    "                    os.path.join(results_path, experiment_name)):\n",
    "                os.makedirs(\n",
    "                    os.path.join(results_path, experiment_name))\n",
    "            file_save = os.path.join(\n",
    "                results_path, experiment_name,\n",
    "                f\"{model_type.name}_{dataset.name}_{distanceType.name}_{tune.get_trial_id()}.csv\")\n",
    "            dfResults.to_csv(path_or_buf=file_save, index=False)\n",
    "\n",
    "        # Log to tune\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save(\n",
    "                (val_model.state_dict(), optimizer.state_dict()),\n",
    "                path)\n",
    "\n",
    "        tune.report(loss=val_loss)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train(config, param, checkpoint_dir=None) -> None:\n",
    "    folder_name = config[\"modelType\"]\n",
    "    if config[\"modelType\"] == ModelType.DCRNN:\n",
    "        folder_name = ModelType.LSTM\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = DatasetHelper(\n",
    "                config[\"sigma\"],\n",
    "                config[\"epsilon\"],\n",
    "                config[\"dataset\"],\n",
    "                param[\"distanceType\"],\n",
    "                param[\"proccessed_data_path\"],\n",
    "                folder_name).get_dataset()\n",
    "    if config[\"modelType\"] == ModelType.STCONV:\n",
    "        model = STConvModel(node_features=2,\n",
    "                                     num_nodes=Graph_get_number_of_nodes_for_dataset(config[\"dataset\"]),\n",
    "                                     hidden_channels=32,\n",
    "                                     kernel_size=1)\n",
    "    elif config[\"modelType\"] == ModelType.LSTM:\n",
    "        model = LSTMModel(node_features=2, hidden_channels=32)\n",
    "    elif config[\"modelType\"] == ModelType.DCRNN:\n",
    "        model = DCRNNModel(node_features=2, hidden_channels=32)\n",
    "    optimizer = Adam(model.parameters(), lr=param[\"learning_rate\"])\n",
    "    scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                           mode='min',\n",
    "                                           factor=0.1,\n",
    "                                           patience=5,\n",
    "                                           threshold=0.00000001,\n",
    "                                           threshold_mode='abs')\n",
    "    train_validate_and_test(param[\"experiment_name\"],model,config[\"modelType\"],optimizer,scheduler,\n",
    "    param[\"EarlyStoppingPatience\"],train_dataset,validation_dataset,test_dataset,param[\"nb_epoch\"],param[\"results_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trail_dirname_creator(trial):\n",
    "        return f\"{trial.config['modelType'].name}_{trial.config['dataset'].name}_{datetime.now().strftime('%d_%m_%Y-%H_%M_%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperParameterTuning(dataset, model, distanceType, experiment_name,\n",
    "    results_ray_path,epsilon_array,sigma_array,proccessed_data_path,results_path):\n",
    "\n",
    "    nb_epoch = 30\n",
    "    grace_period = 30\n",
    "    reduction_factor = 3\n",
    "    scheduler = ASHAScheduler(max_t=nb_epoch,\n",
    "                                grace_period=grace_period,\n",
    "                                reduction_factor=reduction_factor)\n",
    "    learning_rate = 0.01\n",
    "    num_features = 2\n",
    "    EarlyStoppingPatience = 5\n",
    "    nb_epoch\n",
    "    param = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_features\": num_features,\n",
    "        \"EarlyStoppingPatience\": EarlyStoppingPatience,\n",
    "        \"nb_epoch\": nb_epoch,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"distanceType\": distanceType,\n",
    "        \"proccessed_data_path\": proccessed_data_path,\n",
    "        \"results_path\": results_path\n",
    "    }\n",
    "    \n",
    "    config = {\n",
    "        \"epsilon\": tune.choice(epsilon_array),\n",
    "        \"sigma\": tune.choice(sigma_array),\n",
    "        \"modelType\": tune.choice([model]),\n",
    "        \"dataset\" : tune.choice([dataset])\n",
    "    }\n",
    "\n",
    "\n",
    "    directory_experiment_ray = os.path.join(results_ray_path, experiment_name)\n",
    "\n",
    "    num_samples = 16\n",
    "    start_train.__name__ = f\"{model.name}_{dataset.name}_{distanceType.name}\"\n",
    "\n",
    "    result = tune.run(tune.with_parameters(start_train, param=param),\n",
    "                        local_dir=directory_experiment_ray,\n",
    "                        trial_dirname_creator=trail_dirname_creator,\n",
    "                        resources_per_trial={\n",
    "                            \"cpu\": 8,\n",
    "                            \"gpu\": 1\n",
    "                        },\n",
    "                        config=config,\n",
    "                        metric=\"loss\",\n",
    "                        mode=\"min\",\n",
    "                        num_samples=num_samples,\n",
    "                        scheduler=scheduler,\n",
    "                        verbose=0)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial for {} model final validation loss: {}\".format(\n",
    "        model.name, best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(current_directory,\"Results\")\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "results_ray_path = os.path.join(current_directory,\"Results-RAY\")\n",
    "if not os.path.exists(results_ray_path):\n",
    "    os.makedirs(results_ray_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_learn(results_ray_path,path_processed_data,results_path):\n",
    "    experiment_name = f'Experiment_{datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\")}'\n",
    "    directory_experiment_ray = os.path.join(results_ray_path, experiment_name)\n",
    "    if not os.path.exists(directory_experiment_ray):\n",
    "        os.makedirs(directory_experiment_ray)\n",
    "        \n",
    "    for distanceType in DistanceType:\n",
    "        for dataset in [Dataset.Experimental,Dataset.Tiny]:\n",
    "            for model in [ModelType.STCONV]:\n",
    "                hyperParameterTuning(dataset, model, distanceType, experiment_name,\n",
    "                    results_ray_path,EPSILON_ARRAY,SIGMA_ARRAY,path_processed_data,results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learn(results_ray_path,path_processed_data,results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Dissertation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a09a4475c5cc35c3ce3cf73b536683fe1faa908839cea1597f8f0293b34712b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
